commit 87862e0c67cc00aca4018f01e0aa05720acbfeb7
Author: Wangpan <hzwangpan@corp.netease.com>
Date:   Thu May 16 17:59:56 2013 +0800

    decouple nbs and nvs
    
    1. add nbs volume attachment process
    2. add nbs volume detachment process
    3. add nbs volume extension process
    4. add nbs_client.py
    
    Change-Id: Ia9f003f498a3748028ce89879d6eaf5685bcad94

diff --git a/etc/nova/policy.json b/etc/nova/policy.json
index 6ab4160..6f6e4ed 100644
--- a/etc/nova/policy.json
+++ b/etc/nova/policy.json
@@ -7,6 +7,8 @@
     "compute:create": [],
     "compute:create:attach_network": [],
     "compute:create:attach_volume": [],
+    "compute:create:attach_nbs_volume": [],
+    "compute:create:detach_nbs_volume": [],
     "compute:create:forced_host": [["is_admin:True"]],
     "compute:get_all": [],
 
diff --git a/nova/api/openstack/compute/contrib/server_status.py b/nova/api/openstack/compute/contrib/server_status.py
index d9368a4..175230e 100644
--- a/nova/api/openstack/compute/contrib/server_status.py
+++ b/nova/api/openstack/compute/contrib/server_status.py
@@ -14,19 +14,14 @@
 #    License for the specific language governing permissions and limitations
 #    under the License
 
-import datetime
-
-import memcache
 from webob import exc
 
 from nova.api.openstack import extensions
-from nova import db
+from nova import compute
 from nova import exception
 from nova import flags
 from nova.openstack.common import cfg
 from nova.openstack.common import log as logging
-from nova.openstack.common import timeutils
-from nova import utils
 
 
 metadata_opts = [
@@ -39,64 +34,27 @@ FLAGS = flags.FLAGS
 FLAGS.register_opts(metadata_opts)
 LOG = logging.getLogger(__name__)
 authorize = extensions.extension_authorizer('compute', 'server_status')
-MEMCACHE_CLIENT_API = None
 
 
 class ServerStatusController(object):
+    def __init__(self):
+        self.compute_api = compute.API()
 
     def show(self, req, id):
+        context = req.environ['nova.context']
+        authorize(context)
+        LOG.debug(_("Listing server status"), context=context,
+                    instance_uuid=id)
+
         try:
-            context = req.environ['nova.context']
-            authorize(context)
-            LOG.debug(_("Listing server status"))
-            result = self._instance_os_boot_ready(
-                            context, id,
-                            FLAGS.server_heartbeat_period)
+            instance = self.compute_api.get(context, id)
+            result = self.compute_api.instance_os_boot_ready(context,
+                                            instance['uuid'],
+                                            FLAGS.server_heartbeat_period)
             return result
         except exception.NotFound:
             raise exc.HTTPNotFound()
 
-    def _instance_os_boot_ready(self, context, instance_uuid,
-                                heartbeat_period):
-        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
-        status = 'down'
-        interval = heartbeat_period * 1.5
-        try:
-            db.instance_get_by_uuid(context, instance_uuid)
-        except exception.InstanceNotFound:
-            raise exception.NotFound()
-
-        cache_key = str(instance_uuid + '_heart')
-        memcache_client = self._get_memcache_client()
-        if memcache_client is not None:
-            cache_value = memcache_client.get(cache_key)
-            if cache_value:
-                last_heartbeat = datetime.datetime.strptime(
-                                        cache_value,
-                                        '%Y-%m-%d %H:%M:%S')
-                # Timestamps in DB are UTC.
-                elapsed = utils.total_seconds(
-                                timeutils.utcnow() - last_heartbeat)
-                if abs(elapsed) <= interval:
-                    status = 'up'
-        else:
-            msg = _('Store data can not catch')
-            raise exc.HTTPServerError(explanation=msg)
-
-        return {'instance_uuid': instance_uuid,
-                'status': status}
-
-    def _get_memcache_client(self):
-        """Return memcache client"""
-        global MEMCACHE_CLIENT_API
-        if MEMCACHE_CLIENT_API is not None:
-            return MEMCACHE_CLIENT_API
-        else:
-            if FLAGS.memcached_servers:
-                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
-                                                      debug=0)
-            return MEMCACHE_CLIENT_API
-
 
 class Server_status(extensions.ExtensionDescriptor):
     """Add server_status to the Create Server v1.1 API"""
diff --git a/nova/api/openstack/compute/contrib/volumes.py b/nova/api/openstack/compute/contrib/volumes.py
index 9940e30..8d5b41d 100644
--- a/nova/api/openstack/compute/contrib/volumes.py
+++ b/nova/api/openstack/compute/contrib/volumes.py
@@ -15,6 +15,8 @@
 
 """The volumes extension."""
 
+import datetime
+import time
 import webob
 from webob import exc
 from xml.dom import minidom
@@ -27,6 +29,7 @@ from nova import compute
 from nova import exception
 from nova import flags
 from nova.openstack.common import log as logging
+from nova.openstack.common import timeutils
 from nova import utils
 from nova import volume
 from nova.volume import volume_types
@@ -164,6 +167,7 @@ class VolumeController(wsgi.Controller):
     """The Volumes API controller for the OpenStack API."""
 
     def __init__(self):
+        self.compute_api = compute.API()
         self.volume_api = volume.API()
         super(VolumeController, self).__init__()
 
@@ -271,6 +275,61 @@ class VolumeController(wsgi.Controller):
 
         return wsgi.ResponseObject(result, headers=dict(location=location))
 
+    def _extend_nbs_volume(self, req, id, body):
+        """Extend a exists nbs volume."""
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if not self.is_valid_body(body, 'size'):
+            explanation = _("New size is needed in body.")
+            raise exc.HTTPUnprocessableEntity(explanation=explanation)
+        try:
+            instance_uuid = self.compute_api.check_nbs_attached(context, id)
+        except exception.NotFound:
+            explanation = _("volume %s not found.") % id
+            raise exc.HTTPNotFound(explanation=explanation)
+        except exception.Invalid:
+            explanation = _("volume %s not attached.") % id
+            raise webob.exc.HTTPBadRequest(explanation=explanation)
+        size = int(body['size']) # GB
+
+        if instance_uuid:
+            try:
+                # check instance exists
+                instance = self.compute_api.get(context, instance_uuid)
+                # Check os status if instance is 'ACTIVE'
+                if instance['status'] == "ACTIVE":
+                    server_heartbeat_period = FLAGS.get(
+                                                'server_heartbeat_period', 10)
+                    os_status = self.compute_api.instance_os_boot_ready(
+                                                    context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                    if os_status['status'] != "up":
+                        explanation = _("Cannot extend volume while instance "
+                                        "os is starting.")
+                        raise exc.HTTPForbidden(explanation=explanation)
+            except exception.NotFound:
+                explanation = _("Instance %s not found.") % instance_uuid
+                raise exc.HTTPNotFound(explanation=explanation)
+
+            # notify instance about this extension
+            self.compute_api.extend_nbs_volume(context, id, size, instance)
+        else:
+            # call nbs to extend this volume directly
+            self.compute_api.extend_nbs_volume(context, id, size)
+
+        return {}
+
+    # @wsgi.serializers(xml=VolumesTemplate)
+    def update(self, req, id, body):
+        """Extend a exists volume."""
+        if FLAGS.ebs_backend == 'nbs':
+            return self._extend_nbs_volume(req, id, body)
+        else:
+            raise exc.HTTPBadRequest()
+
+
 
 def _translate_attachment_detail_view(volume_id, instance_uuid, mountpoint):
     """Maps keys for attachment details view."""
@@ -375,9 +434,68 @@ class VolumeAttachmentController(wsgi.Controller):
             instance['uuid'],
             assigned_mountpoint)}
 
+    def _attach_nbs_volume(self, req, server_id, body):
+        """Attach a nbs volume to an instance."""
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if not self.is_valid_body(body, 'volumeAttachment'):
+            explanation = _("Paramater volumeAttachment is needed in body.")
+            raise exc.HTTPUnprocessableEntity(explanation=explanation)
+
+        volume_id = body['volumeAttachment']['volumeId']
+
+        msg = _("Attach nbs volume %s to instance %s") % (volume_id, server_id)
+        LOG.audit(msg, context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+            # TODO(wangpan): if the instance forbid to attach nbs volume,
+            #                raise exception.
+            # Check os status if instance is 'ACTIVE'
+            if instance['status'] == "ACTIVE":
+                server_heartbeat_period = FLAGS.get('server_heartbeat_period',
+                                                    10)
+                os_status = self.compute_api.instance_os_boot_ready(context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                if os_status['status'] != "up":
+                    explanation = _("Cannot attach volume while instance os "
+                                    "is starting.")
+                    raise exc.HTTPForbidden(explanation=explanation)
+            device = self.compute_api.attach_nbs_volume(context, instance,
+                                                        volume_id)
+        except exception.NotFound:
+            explanation = _("Instance %s not found.") % server_id
+            raise exc.HTTPNotFound(explanation=explanation)
+
+        # The attach is async
+        attachment = {}
+        attachment['requestId'] = context.request_id
+        attachment['instanceId'] = instance['uuid']
+        attachment['volumeId'] = volume_id
+        attachment['device'] = device
+        attachment['status'] = 0
+        attachment['attachTime'] = long(time.time())
+
+        # NOTE(justinsb): And now, we have a problem...
+        # The attach is async, so there's a window in which we don't see
+        # the attachment (until the attachment completes).  We could also
+        # get problems with concurrent requests.  I think we need an
+        # attachment state, and to write to the DB here, but that's a bigger
+        # change.
+        # For now, we'll probably have to rely on libraries being smart
+
+        # TODO(justinsb): How do I return "accepted" here?
+        return {'attachment': attachment}
+
     @wsgi.serializers(xml=VolumeAttachmentTemplate)
     def create(self, req, server_id, body):
         """Attach a volume to an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._attach_nbs_volume(req, server_id, body)
+
         context = req.environ['nova.context']
         authorize(context)
 
@@ -420,8 +538,61 @@ class VolumeAttachmentController(wsgi.Controller):
         """Update a volume attachment.  We don't currently support this."""
         raise exc.HTTPBadRequest()
 
+    def _detach_nbs_volume(self, req, server_id, id):
+        """Detach a nbs volume from an instance."""
+
+        context = req.environ['nova.context']
+        authorize(context)
+
+        volume_id = id
+        LOG.audit(_("Detach nbs volume %s from instance %s"),
+                    (volume_id, server_id), context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+        except exception.NotFound:
+            explanation = _("Instance %s not found.") % server_id
+            raise exc.HTTPNotFound(explanation=explanation)
+
+        bdms = self.compute_api.get_instance_bdms(context, instance)
+
+        if not bdms:
+            LOG.debug(_("Instance %s is not attached."), server_id)
+            explanation = _("volume %s is not attached.") % volume_id
+            raise exc.HTTPNotFound(explanation=explanation)
+
+        found = False
+        too_short_interval = False
+        for bdm in bdms:
+            if bdm['volume_id'] == volume_id:
+                # FIXME(wangpan): we check the update time of device here
+                #                 to avoid attaching/detaching too frequently
+                #                 (vm may get exception if do that).
+                if ((timeutils.utcnow() - bdm['updated_at'])
+                    < datetime.timedelta(seconds=FLAGS.attch_detach_interval)):
+                    too_short_interval = True
+                    break
+                self.compute_api.detach_nbs_volume(context,
+                    instance, volume_id)
+                found = True
+                break
+
+        if too_short_interval:
+            explanation = _("Too short interval after attaching to detach "
+                            "volume %s." % volume_id)
+            raise exc.HTTPForbidden(explanation=explanation)
+        if not found:
+            explanation = _("volume %s is not attached.") % volume_id
+            raise exc.HTTPNotFound(explanation=explanation)
+        else:
+            return webob.Response(status_int=202)
+
     def delete(self, req, server_id, id):
         """Detach a volume from an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._detach_nbs_volume(req, server_id, id)
+
         context = req.environ['nova.context']
         authorize(context)
 
diff --git a/nova/compute/api.py b/nova/compute/api.py
index bd417fe..b071918 100644
--- a/nova/compute/api.py
+++ b/nova/compute/api.py
@@ -22,14 +22,18 @@
 networking and storage of VMs, and compute hosts on which they run)."""
 
 import base64
+import datetime
 import functools
 import re
 import string
 import time
 import urllib
 
+import memcache
+
 from nova import block_device
 from nova.compute import instance_types
+from nova.compute import nbs_client
 from nova.compute import power_state
 from nova.compute import rpcapi as compute_rpcapi
 from nova.compute import task_states
@@ -63,6 +67,7 @@ flags.DECLARE('consoleauth_topic', 'nova.consoleauth')
 
 MAX_USERDATA_SIZE = 65535
 QUOTAS = quota.QUOTAS
+MEMCACHE_CLIENT_API = None
 
 
 def check_instance_state(vm_state=None, task_state=(None,)):
@@ -146,6 +151,7 @@ class API(base.Base):
 
         self.network_api = network_api or network.API()
         self.volume_api = volume_api or volume.API()
+        self.nbs_api = nbs_client.API()
         self.security_group_api = security_group_api or SecurityGroupAPI()
         self.sgh = importutils.import_object(FLAGS.security_group_handler)
         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
@@ -2104,6 +2110,118 @@ class API(base.Base):
         """Inject network info for the instance."""
         self.compute_rpcapi.inject_network_info(context, instance=instance)
 
+    def _get_memcache_client(self):
+        """Return memcache client"""
+        global MEMCACHE_CLIENT_API
+        if MEMCACHE_CLIENT_API is not None:
+            return MEMCACHE_CLIENT_API
+        else:
+            if FLAGS.memcached_servers:
+                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
+                                                      debug=0)
+            return MEMCACHE_CLIENT_API
+
+    def instance_os_boot_ready(self, context, instance_uuid, heartbeat_period):
+        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
+        status = 'down'
+        interval = heartbeat_period * 1.5
+
+        cache_key = str(instance_uuid + '_heart')
+        memcache_client = self._get_memcache_client()
+        if memcache_client is not None:
+            cache_value = memcache_client.get(cache_key)
+            if cache_value:
+                last_heartbeat = datetime.datetime.strptime(
+                                        cache_value,
+                                        '%Y-%m-%d %H:%M:%S')
+                # Timestamps in DB are UTC.
+                elapsed = utils.total_seconds(
+                                timeutils.utcnow() - last_heartbeat)
+                if abs(elapsed) <= interval:
+                    status = 'up'
+        else:
+            msg = _('Store data can not catch')
+            raise exc.HTTPServerError(explanation=msg)
+
+        return {'instance_uuid': instance_uuid,
+                'status': status}
+
+    def check_nbs_attached(context, volume_id, instance_uuid=None):
+        """
+        """
+        volumes = self.nbs_api.get(context, volume_id)['volumes']
+        if len(volumes) == 0:
+            raise exception.VolumeNotFound(volume_id=volume_id)
+
+        volume = volumes[0]
+        if volume.get('status', None) != 'attachedVM':
+            msg = _("Volume must be attached in order to detach.")
+            raise exception.InvalidVolume(reason=msg)
+
+        if (instance_uuid is not None and
+                volume.get('instance_uuid', None) != instance_uuid):
+            raise exception.VolumeUnattached(volume_id=volume_id)
+
+        return volume.get('instance_uuid', None)
+
+    @check_instance_lock
+    def extend_nbs_volume(self, context, volume_id, size, instance=None):
+        """ """
+        if instance is None:
+            # call nbs to extend the volume, we do nothing
+            self.nbs_api.extend(context, volume_id, size)
+        else:
+            self.compute_rpcapi.extend_nbs_volume(context, instance=instance,
+                        volume_id=volume_id, size=size, device=None)
+
+    @wrap_check_policy
+    @check_instance_lock
+    def attach_nbs_volume(self, context, instance, volume_id):
+        """Attach an existing volume to an existing instance."""
+        # NOTE(vish): This is done on the compute host because we want
+        #             to avoid a race where two devices are requested at
+        #             the same time. When db access is removed from
+        #             compute, the bdm will be created here and we will
+        #             have to make sure that they are assigned atomically.
+
+        # Check volume exists and is available to attach
+        volumes = self.nbs_api.get(context, volume_id)['volumes']
+        if len(volumes) == 0:
+            raise exception.VolumeNotFound(volume_id=volume_id)
+
+        volume = volumes[0]
+        if int(volume.get('status', None)) != 'available':
+            reason = _("Volume %s is not available") % volume_id
+            raise exception.InvalidVolume(reason=reason)
+
+        device = self.compute_rpcapi.get_device_for_nbs_volume(
+                        context, instance=instance)
+        if device is None:
+            # can't find a slot or target dev to attach
+            LOG.error(_("Cannot find a free slot or target dev to attach "
+                        "volume %s") % volume_id,
+                        context=context, instance=instance)
+            raise exception.NoFreeDevice()
+
+        # deal with race condition here,
+        # such as attaching a volume two times at a short interval
+        db_vol = self.db.block_device_mapping_get_all_by_volume(context,
+                                                                volume_id)
+        if len(db_vol) > 0:
+            self.db.block_device_mapping_destroy_by_instance_and_device(
+                        context, instance['uuid'], device)
+            reason = _("Volume %s is in use") % volume_id
+            raise exception.InvalidVolume(reason=reason)
+        else:
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(device),
+                      'volume_id': volume_id}
+            self.db.block_device_mapping_update_or_create(context, values)
+        self.compute_rpcapi.attach_nbs_volume(context, instance=instance,
+                volume_id=volume_id, device=device)
+
+        return device['real_path']
+
     @wrap_check_policy
     @check_instance_lock
     def attach_volume(self, context, instance, volume_id, device=None):
@@ -2135,6 +2253,15 @@ class API(base.Base):
         return device
 
     @check_instance_lock
+    def detach_nbs_volume(self, context, instance, volume_id):
+        """Detach a volume from an instance."""
+        self.check_nbs_attached(context, volume_id, instance['uuid'])
+
+        check_policy(context, 'detach_nbs_volume', instance)
+        self.compute_rpcapi.detach_nbs_volume(context, instance=instance,
+                volume_id=volume_id)
+
+    @check_instance_lock
     def _detach_volume(self, context, instance, volume_id):
         check_policy(context, 'detach_volume', instance)
 
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index e863e73..3c9acb7 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -52,6 +52,7 @@ from nova import block_device
 from nova import compute
 from nova.compute import ebs_client
 from nova.compute import instance_types
+from nova.compute import nbs_client
 from nova.compute import power_state
 from nova.compute import resource_tracker
 from nova.compute import rpcapi as compute_rpcapi
@@ -357,6 +358,7 @@ class ComputeManager(manager.SchedulerDependentManager):
         self.network_api = network.API()
         self.volume_api = volume.API()
         self.ebs_api = ebs_client.API()
+        self.nbs_api = nbs_client.API()
         self._last_host_check = 0
         self._last_bw_usage_poll = 0
         self._last_info_cache_heal = 0
@@ -466,6 +468,38 @@ class ComputeManager(manager.SchedulerDependentManager):
         else:
             LOG.warning(_('get instance storage device for ioqos failed!'))
 
+    def _check_and_wait_nbs_vol(self, count, bdms):
+        """
+        Check if nbs volumes disappeared, if that wait for nbs volumes appear.
+        """
+        factor = (count + 1) ** 2
+        wait_seconds = 0
+        disappeared_host_devs = []
+
+        for bdm in bdms:
+            host_dev = jsonutils.loads(bdm['connect_info']).get('host_dev')
+            if host_dev is not None and not os.path.exists(host_dev):
+                disappeared_host_devs.append(host_dev)
+
+        if len(disappeared_host_devs) == 0:
+            return
+
+        wait_seconds = FLAGS.nbs_boot_wait_timeout / factor
+        if wait_seconds == 0:
+            return
+
+        start = time.time()
+        while len(disappeared_host_devs) > 0:
+            LOG.info(_("Wait for nbs volume %s 3 seconds") % host_dev)
+            # FIXME(wangpan): sticked in 3s here.
+            time.sleep(3)
+            disappeared_host_devs = [host_dev for host_dev in
+                        disappeared_host_devs if not os.path.exists(host_dev)]
+            if time.time() - start >= wait_seconds:
+                break
+        if len(disappeared_host_devs) > 0:
+            LOG.warning(_("Nbs volume disappeared %s") % disappeared_host_devs)
+
     def init_host(self):
         """Initialization for a standalone compute service."""
         self.driver.init_host(host=self.host)
@@ -517,6 +551,7 @@ class ComputeManager(manager.SchedulerDependentManager):
                     block_device_info = \
                         self._get_instance_volume_block_device_info(
                             context, instance['uuid'])
+                    self._check_and_wait_nbs_vol(count, block_device_info)
 
                     try:
                         self.driver.resume_state_on_host_boot(
@@ -2878,6 +2913,23 @@ class ComputeManager(manager.SchedulerDependentManager):
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
+    def get_device_for_nbs_volume(self, context, instance, used_dev=None):
+
+        @utils.synchronized(instance['uuid'])
+        def do_reserve():
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(result)}
+            self.db.block_device_mapping_create(context, values)
+            return result
+        return do_reserve()
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
     def reserve_block_device_name(self, context, instance, device):
 
         @utils.synchronized(instance['uuid'])
@@ -2892,6 +2944,184 @@ class ComputeManager(manager.SchedulerDependentManager):
             return result
         return do_reserve()
 
+    def _get_host_ip_by_ifname(self, ifname):
+        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        try:
+            import fcntl
+            ip = socket.inet_ntoa(fcntl.ioctl(
+                                              s.fileno(),
+                                              0x8915,  # SIOCGIFADDR
+                                              struct.pack('256s', ifname[:15])
+                                              )[20:24])
+            return ip
+        except Exception:
+            return None
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def extend_nbs_volume(self, context, instance, volume_id, size, device):
+        """Extend a nbs volume which has been attached on an instance."""
+        # Get attached device from nova db
+        if device is None:
+            bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                                volume_id)
+            device = jsonutils.loads(bdm['device_name'])
+        try:
+            # call nbs to extend this volume firstly
+            host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+            result = self.nbs_api.extend(context, volume_id, size, host_ip)
+            if result is None:
+                raise exception.NbsException()
+
+            # wait for nbs extending finish
+            succ = self.wait_for_extended(context, volume_id, size)
+            if not succ:
+                raise exception.NbsTimeout()
+
+        except Exception:
+            # notify nbs extend failed
+            self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                    'extend', False,
+                                                    device['real_path'],
+                                                    host_ip,
+                                                    instance['uuid'])
+
+        try:
+            return self.driver.extend_nbs_volume(instance, device, size)
+        except Exception:
+            # notify nbs extend failed
+            self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                    'extend', False,
+                                                    device['real_path'],
+                                                    host_ip,
+                                                    instance['uuid'])
+        else:
+            # notify nbs extend OK
+            self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                    'extend', True,
+                                                    device['real_path'],
+                                                    host_ip,
+                                                    instance['uuid'])
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def attach_nbs_volume(self, context, volume_id, device, instance):
+        """Attach a nbs volume to an instance."""
+        # TODO(wangpan): if this host is forbidden to attach nbs volume,
+        #                an exception needs to be raised.
+        try:
+            return self._attach_nbs_volume(context, volume_id,
+                                           device, instance)
+        except Exception:
+            with excutils.save_and_reraise_exception():
+                self.db.block_device_mapping_destroy_by_instance_and_device(
+                        context, instance.get('uuid'), jsonutils.dumps(device))
+
+    def _attach_nbs_volume(self, context, volume_id, device, instance):
+        # context = context.elevated()
+        LOG.audit(_('Attaching nbs volume %(volume_id)s to %(device)s'),
+                  locals(), context=context, instance=instance)
+        try:
+            # call nbs to attach volume to this host
+            host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+            result = self.nbs_api.attach(context, volume_id, host_ip)
+            if result is None:
+                raise exception.NbsException()
+
+            # check volume status, wait for nbs attaching finish
+            succ = self.nbs_api.wait_for_attached(context, volume_id)
+            if not succ:
+                raise exception.NbsTimeout()
+
+            # get host dev path and QoS params from nbs
+            result = self.nbs_api.get_host_dev_and_qos_info(context, volume_id)
+            if result is None:
+                raise exception.NbsException()
+
+            host_dev = result['devicePath']
+            qos_info = {'iotune_total_bytes': int(result['maxBandWith']),
+                        'iotune_total_iops': int(result['maxIOPS'])}
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to connect to volume %(volume_id)s "
+                        "while attaching at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+                self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                        'attach', False,
+                                                        device['real_path'],
+                                                        host_ip,
+                                                        instance['uuid'])
+
+        @utils.synchronized(instance['uuid'])
+        def get_new_device(used_dev):
+            """
+            We get a new free slot and target dev by calling libvirt driver.
+            """
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(result)}
+            self.db.block_device_mapping_update_or_create(context, values)
+            return result
+
+        try:
+            used_dev = {'dev': [], 'slot': []}
+            retry = True
+            while retry:
+                retry, used = self.driver.attach_nbs_volume(instance['name'],
+                                    device, host_dev, qos_info, volume_id)
+                if retry:
+                    if used['dev']:
+                        used_dev['dev'].append(used['dev'])
+                    elif used['slot']:
+                        used_dev['slot'].append(used['slot'])
+                    device = get_new_device(used_dev)
+                    if device is None:
+                        # can't find a slot or target dev to attach
+                        LOG.error(_("Cannot find a free slot or target dev "
+                                    "to attach volume %s") % volume_id,
+                                    context=context, instance=instance)
+                        raise exception.NoFreeDevice()
+                    LOG.info(_("Retry to attach nbs volume %s at %s"
+                            % (volume_id, device),
+                            context=context, instance=instance)
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to attach nbs volume %(volume_id)s "
+                        "at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+                self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                        'attach', False,
+                                                        device['real_path'],
+                                                        host_ip,
+                                                        instance['uuid'])
+        else:
+            # notify nbs attach successfully
+            self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                    'attach', True, device['real_path'],
+                                    host_ip, instance['uuid'])
+
+        values = {
+            'instance_uuid': instance['uuid'],
+            'connection_info': jsonutils.dumps({'host_dev': host_dev,
+                                                'qos_info': qos_info}),
+            'device_name': jsonutils.dumps(device),
+            'delete_on_termination': False,
+            'virtual_name': None,
+            'snapshot_id': None,
+            'volume_id': volume_id,
+            'volume_size': None,
+            'no_device': None}
+        self.db.block_device_mapping_update_or_create(context, values)
+
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
@@ -2956,6 +3186,47 @@ class ComputeManager(manager.SchedulerDependentManager):
             'no_device': None}
         self.db.block_device_mapping_update_or_create(context, values)
 
+    def _detach_nbs_volume(self, context, instance, bdm):
+        """Do the actual driver detach using block device mapping."""
+        mp = jsonutils.loads(bdm['device_name'])['mountpoint']
+        volume_id = bdm['volume_id']
+
+        LOG.audit(_('Detach volume %(volume_id)s from mountpoint %(mp)s'),
+                  locals(), context=context, instance=instance)
+
+        try:
+            self.driver.detach_nbs_volume(instance['name'], mp)
+        except Exception as ex:  # pylint: disable=W0702
+            msg = _("Faild to detach volume %(volume_id)s from %(mp)s")
+            LOG.warning(msg % locals(), context=context, instance=instance)
+            raise ex
+        else:
+            # FIXME(wangpan): we sleep here to avoid attaching/detaching
+            #                 too frequently(vm may get exception if do that).
+            time.sleep(FLAGS.attch_detach_interval)
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def detach_nbs_volume(self, context, volume_id, instance):
+        """Detach a nbs volume from an instance."""
+        bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                            volume_id)
+        try:
+            self._detach_nbs_volume(context, instance, bdm)
+        except Exception:
+            # FIXME(wangpan): if libvirt raise an exception while detaching,
+            #                 we ignore it and always call nbs to detach it.
+            pass
+        else:
+            self.db.block_device_mapping_destroy_by_instance_and_volume(
+                    context, instance['uuid'], volume_id)
+
+        # call nbs to detach volume from host
+        host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+        # we don't check the result of nbs detachment
+        self.nbs_api.detach(context, volume, host_ip)
+
     def _detach_volume(self, context, instance, bdm):
         """Do the actual driver detach using block device mapping."""
         mp = bdm['device_name']
diff --git a/nova/compute/nbs_client.py b/nova/compute/nbs_client.py
new file mode 100644
index 0000000..3b86584
--- /dev/null
+++ b/nova/compute/nbs_client.py
@@ -0,0 +1,320 @@
+# author: hzwangpan@corp.netease.com
+# client of Nbs API for nova to check and detach all volume(s) when VM is
+# going to shutdown, make sure the volume status in Nbs is identical with
+# the status in nova.
+
+import httplib
+import json
+import time
+
+from nova import flags
+from nova.openstack.common import cfg
+from nova.openstack.common import log as logging
+from nova.openstack.common.notifier import api as notifier_api
+
+
+FLAGS = flags.FLAGS
+LOG = logging.getLogger(__name__)
+
+
+class API():
+    def __init__(self):
+        self._nbs_api_server = FLAGS.nbs_api_server or ""
+        if "http://" in self._nbs_api_server:
+            self._nbs_api_server = self._nbs_api_server.replace("http://", "")
+        if "/" in self._nbs_api_server:
+            self._nbs_api_server = self._nbs_api_server.replace("/", "")
+
+    def get(self, context, volume_id):
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=DescribeVolumes"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id))
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def attach(self, context, volume_id, host_ip):
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=AttachVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&HostIp=" + str(host_ip))
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def detach(self, context, volume_id, host_ip):
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=DetachVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&HostIp=" + host_ip)
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def extend(self, context, volume_id, size, host_ip=None):
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not size:
+            LOG.warn(_("size is should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=ExtendVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&Size=" + str(size))
+        if host_ip:
+            url += "&HostIp=" + host_ip
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def wait_for_attached(self, context, volume_id, check_interval=3):
+        # FIXME:How to deal with the timeout situation?
+        start = time.time()
+        times = 1
+        while (time.time() - start_time < FLAGS.nbs_attach_wait_timeout):
+            volumes = self.get(context, volume_id)['volumes']
+            # if we get nothing or wrong content from the Nbs server,
+            # we don't retry and return immediately.
+            if len(volumes) == 0:
+                LOG.warn(_("get nothing from nbs server, return now, "
+                           "times: %d") % times, context=context)
+                return False
+
+            volume = volumes[0]
+            if volume.get("status", None) == "attached":
+                return True
+
+            LOG.info(_("sleep %ds and retry to check volume's status, "
+                    "times: %d") % (check_interval, times), context=context)
+            time.sleep(check_interval)
+            times += 1
+        LOG.warn(_("volume %s can not be attached successfully after %ds") %
+                 (volume_id, FLAGS.nbs_attach_wait_timeout), context=context)
+        return False
+
+    def wait_for_extended(self, context, volume_id, expected_size,
+                            check_interval=3):
+        # FIXME:How to deal with the timeout situation?
+        start = time.time()
+        times = 1
+        while (time.time() - start_time < FLAGS.nbs_extend_wait_timeout):
+            volumes = self.get(context, volume_id)['volumes']
+            # if we get nothing or wrong content from the Nbs server,
+            # we don't retry and return immediately.
+            if len(volumes) == 0:
+                LOG.warn(_("get nothing from nbs server, return now, "
+                           "times: %d") % times, context=context)
+                return False
+
+            volume = volumes[0]
+            if int(volume.get("size", -1)) == expected_size:
+                return True
+
+            LOG.info(_("sleep %ds and retry to check volume's status, "
+                    "times: %d") % (check_interval, times), context=context)
+            time.sleep(check_interval)
+            times += 1
+        LOG.warn(_("volume %s can't be extended successfully after %ds") %
+                 (volume_id, FLAGS.nbs_extend_wait_timeout), context=context)
+        return False
+
+    def get_host_dev_and_qos_info(context, volume_id, host_ip):
+        """
+        """
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=GetVolumeQos"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&HostIp=" + str(host_ip))
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def notify_nbs_libvirt_result(context, volume_id, operation, result,
+                                    device, host_ip, instance_uuid):
+        """
+        """
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not operation:
+            LOG.warn(_("operation should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if result not in (True, False):
+            LOG.warn(_("result should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not device:
+            LOG.warn(_("device should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not instance_uuid:
+            LOG.warn(_("instance uuid should be given, do nothing "
+                        "and return now"), context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=NotifyState"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&OperateType=" + str(operation)
+                + "&Device=" + str(device)
+                + "&HostIp=" + str(host_ip)
+                + "&InstanceId=" + str(instance_uuid))
+        if result:
+            url += "&OperateState=" + "success"
+        else:
+            url += "&OperateState=" + "fail"
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def _request(self, context, method, url, params, headers):
+        if not self._nbs_api_server:
+            LOG.warn(_("nbs_api_server is null, can't connect to it"))
+            return None
+
+        unified_log_id = None
+        unified_log_seq = None
+        if context:
+            unified_log_id = context.to_dict().get('unified_log_id', None)
+            unified_log_seq = context.to_dict().get('unified_log_seq', None)
+        if unified_log_id and unified_log_seq:
+            log_seq_nums = unified_log_seq.split('.')
+            log_seq_nums[-1] = str(int(log_seq_nums[-1]) + 1)
+            new_log_seq = '.'.join(log_seq_nums)
+            context.unified_log_seq = new_log_seq
+
+            url = url + "&LogId=" + str(unified_log_id)
+            url = url + "&LogSeq=" + str(new_log_seq)
+
+        full_url = self._nbs_api_server + url
+
+        LOG.info(_("send request to %(full_url)s, method: %(method)s, "
+                   "body: %(params)s, headers: %(headers)s") % locals(),
+                   context=context)
+        try:
+            nbs_conn = httplib.HTTPConnection(self._nbs_api_server)
+        except Exception, ex:
+            LOG.error(_("exception occurs when connect to nbs server, "
+                        "error msg: %s") % str(ex), context=context)
+            self._notify_NBS_connection_failure(context, self._nbs_api_server)
+            return None
+
+        try:
+            nbs_conn.request(method, url, params, headers)
+            resp = nbs_conn.getresponse()
+            if resp:
+                if resp.status == 200:
+                    data = json.loads(resp.read())
+                    request_id = data["requestId"]
+                    LOG.info(_("request id: %(request_id)s, data: "
+                               "%(data)s") % locals(), context=context)
+                    return data
+                else:
+                    err_code = resp.status
+                    err_reason = resp.reason
+                    LOG.error(_("error occurs when contact to nbs server, "
+                                "error code: %(err_code)d, "
+                                "reason: %(err_reason)s") % locals(),
+                                context=context)
+                    self._notify_NBS_connection_failure(context, full_url)
+                    return None
+            else:
+                LOG.error(_("nbs server doesn't return any response"),
+                            context=context)
+                self._notify_NBS_connection_failure(context, full_url)
+                return None
+
+        except Exception, ex:
+            LOG.error(_("exception occurs when send request to nbs server, "
+                        "error msg: %s") % str(ex), context=context)
+            self._notify_NBS_connection_failure(context, full_url)
+            return None
+        finally:
+            nbs_conn.close()
+
+    def _notify_NBS_connection_failure(self, context, url):
+        """
+        Send a message to notification about NBS connection failure.
+        """
+        try:
+            LOG.info(_('notify NBS connection failure'))
+            payload = dict({'url': url})
+            notifier_api.notify(context,
+                                notifier_api.publisher_id('api_nbs'),
+                                'api_nbs.nvs_connect_nbs_failure',
+                                notifier_api.ERROR, payload)
+        except Exception:
+            LOG.exception(_('notification module error when do notifying '
+                            'NVS connect NBS failed.'))
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index 0cd2668..327ca16 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -170,6 +170,19 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 instance=instance_p, network_id=network_id),
                 topic=_compute_topic(self.topic, ctxt, None, instance))
 
+    def extend_nbs_volume(self, ctxt, instance, device=None):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('extend_nbs_volume',
+                instance=instance_p, volume_id=volume_id,
+                size=size, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
+    def attach_nbs_volume(self, ctxt, instance, volume_id, device):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('attach_nbs_volume',
+                instance=instance_p, volume_id=volume_id, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def attach_volume(self, ctxt, instance, volume_id, mountpoint):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('attach_volume',
@@ -210,6 +223,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 reservations=reservations),
                 topic=_compute_topic(self.topic, ctxt, host, instance))
 
+    def detach_nbs_volume(self, ctxt, instance, volume_id):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('detach_nbs_volume',
+                instance=instance_p, volume_id=volume_id),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def detach_volume(self, ctxt, instance, volume_id):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('detach_volume',
@@ -458,6 +477,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
         topic = _compute_topic(self.topic, ctxt, host, None)
         return self.call(ctxt, self.make_msg('get_host_uptime'), topic)
 
+    def get_device_for_nbs_volume(self, ctxt, instance):
+        instance_p = jsonutils.to_primitive(instance)
+        return self.call(ctxt, self.make_msg('get_device_for_nbs_volume',
+                instance=instance_p),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def reserve_block_device_name(self, ctxt, instance, device):
         instance_p = jsonutils.to_primitive(instance)
         return self.call(ctxt, self.make_msg('reserve_block_device_name',
diff --git a/nova/compute/utils.py b/nova/compute/utils.py
index a8a0a65..f331510 100644
--- a/nova/compute/utils.py
+++ b/nova/compute/utils.py
@@ -55,7 +55,6 @@ def add_instance_fault_from_exc(context, instance_uuid, fault, exc_info=None):
     }
     db.instance_fault_create(context, values)
 
-
 def get_device_name_for_instance(context, instance, device):
     """Validates (or generates) a device name for instance.
 
diff --git a/nova/db/api.py b/nova/db/api.py
index 68b3cf2..c2056cd 100644
--- a/nova/db/api.py
+++ b/nova/db/api.py
@@ -1323,6 +1323,10 @@ def block_device_mapping_get_all_by_instance(context, instance_uuid):
     return IMPL.block_device_mapping_get_all_by_instance(context,
                                                          instance_uuid)
 
+def block_device_mapping_get_all_by_volume(context, volume_id):
+    """Get all block device mapping by volume"""
+    return IMPL.block_device_mapping_get_all_by_instance(context, volume_id)
+
 
 def block_device_mapping_destroy(context, bdm_id):
     """Destroy the block device mapping."""
diff --git a/nova/db/sqlalchemy/api.py b/nova/db/sqlalchemy/api.py
index fef3f99..59fd96d 100644
--- a/nova/db/sqlalchemy/api.py
+++ b/nova/db/sqlalchemy/api.py
@@ -3741,6 +3741,12 @@ def block_device_mapping_get_all_by_instance(context, instance_uuid):
                  filter_by(instance_uuid=instance_uuid).\
                  all()
 
+@require_context
+def block_device_mapping_get_all_by_volume(context, volume_id):
+    return _block_device_mapping_get_query(context).\
+                 filter_by(instance_uuid=volume_id).\
+                 all()
+
 
 @require_context
 def block_device_mapping_destroy(context, bdm_id):
diff --git a/nova/exception.py b/nova/exception.py
index 2c3e7bb..274646d 100644
--- a/nova/exception.py
+++ b/nova/exception.py
@@ -378,6 +378,10 @@ class DevicePathInUse(Invalid):
     message = _("The supplied device path (%(path)s) is in use.")
 
 
+class NoFreeDevice(Invalid):
+    message = _("Cannot find a free device to attach nbs volume.")
+
+
 class DeviceIsBusy(Invalid):
     message = _("The supplied device (%(device)s) is busy.")
 
@@ -1141,6 +1145,18 @@ class EcuKeyError(NovaException):
     message = _("Key 'ecus_per_vcpu:' may not be set")
 
 
+class NbsException(NovaException):
+    message = _("Nbs error occurred")
+
+
+class NbsTimeout(NovaException):
+    message = _("Nbs operation timeout")
+
+
+class NotSupported(NovaException):
+    message = _("Operation is not supported currently")
+
+
 class TcInvalid(Invalid):
     message = _("%(err)s")
 
diff --git a/nova/flags.py b/nova/flags.py
index c0112ce..e889f23 100644
--- a/nova/flags.py
+++ b/nova/flags.py
@@ -230,6 +230,41 @@ resize_opts = [
 
 FLAGS.register_opts(resize_opts)
 
+nbs_opts = [
+    cfg.StrOpt('ebs_backend',
+                default='nbs',
+                help='the backend type of ebs service, may be nbs or cinder'),
+    cfg.StrOpt('nbs_api_server',
+                default=None,
+                help='the host and port of nbs server'),
+    cfg.StrOpt('nbs_prefix_url',
+                default='EBS',
+                help='the url prefix of nbs server, the final url should like '
+                 'this: nbs_api_server/nbs_prefix_url/?Action=XX&YY=ZZ...'),
+    cfg.StrOpt('host_ip_ifname',
+               default='br100',
+               help='network device to get intranet ip address for nbs'),
+    cfg.IntOpt('nbs_boot_wait_timeout',
+               default=120,
+               help='nbs volume wait timeout during init host(in second)'),
+    cfg.IntOpt('nbs_attach_wait_timeout',
+               default=30,
+               help='nbs volume wait timeout during attachment(in second)'),
+    cfg.IntOpt('nbs_extend_wait_timeout',
+               default=30,
+               help='nbs volume wait timeout during extension(in second)'),
+    cfg.IntOpt('attch_detach_interval',
+               default=5,
+               help='interval of nbs volume between attachment and '
+                    'detachment(in second)'),
+    cfg.StrOpt('nbs_mountpoint_prefix',
+               default='/dev/nbs/xd',
+               help='mount point prefix of nbs disk displays in vm, may like '
+                    '/dev/nbs/xdaz'),
+]
+
+FLAGS.register_opts(nbs_opts)
+
 global_opts = [
     cfg.StrOpt('my_ip',
                default=_get_my_ip(),
diff --git a/nova/virt/driver.py b/nova/virt/driver.py
index 8be6ff7..293c150 100644
--- a/nova/virt/driver.py
+++ b/nova/virt/driver.py
@@ -233,10 +233,23 @@ class ComputeDriver(object):
         # TODO(Vek): Need to pass context in for access to auth_token
         raise NotImplementedError()
 
+    def extend_nbs_volume(self, instance_name, device, size):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
     def attach_volume(self, connection_info, instance_name, mountpoint):
         """Attach the disk to the instance at mountpoint using info"""
         raise NotImplementedError()
 
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        """Detach the disk attached to the instance"""
+        raise NotImplementedError()
+
     def detach_volume(self, connection_info, instance_name, mountpoint):
         """Detach the disk attached to the instance"""
         raise NotImplementedError()
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 5bd7529..80f5723 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -734,6 +734,144 @@ class LibvirtDriver(driver.ComputeDriver):
         method = getattr(driver, method_name)
         return method(connection_info, *args, **kwargs)
 
+    def _slot_to_disk(self, slot):
+        ascii_table = "abcdefghijklmnopqrstuvwxyz"
+        nums = slot
+        nbs_disk = ''
+        while nums > 0:
+            value = nums % 26
+            ch = ascii_table[value:value + 1]
+            nbs_disk = ch + nbs_disk
+            nums = nums / 26
+        return nbs_disk
+
+    # FIXME(wangpan): we find that only slot id is non-zero in pci address,
+    #                 such as: <address type='pci'
+    #                           domain='0x0000'
+    #                           bus='0x00'
+    #                           slot='0x03'
+    #                           function='0x0'/>
+    #                 so we only find a free slot of domain.
+    def _find_free_dev_and_pcislot(self, instance_name, used_dev):
+        virt_dom = self._lookup_by_name(instance_name)
+        dom_xml = virt_dom.XMLDesc(0)
+        doc = etree.fromstring(dom_xml)
+        devices = doc.findall('./devices/')
+        used_pci_slot_ids = [int(device.find('address').attrib['slot'], 16)
+                                for device in devices
+                                    if device.find('address') is not None]
+        disks = doc.findall('./devices/disk')
+        used_disk_devs = [disk.find('target').attrib['dev'] for disk in disks]
+
+        free_slot = None
+        free_dev = None
+
+        # slot id range is 0-31
+        for id in range(31, 0, -1):
+            if id not in used_pci_slot_ids and id not in used_dev['slot']:
+                free_slot = id
+
+        # vd[c-z]
+        for i in range(99, 123):
+            dev = 'vd' + chr(i)
+            if dev not in used_disk_devs and dev not in used_dev['dev']:
+                free_dev = dev
+
+        return (free_slot, free_dev)
+
+
+    def get_device_for_nbs_volume(instance_name, used_dev):
+        """Generates a device name for attaching nbs volume"""
+        free_slot, free_dev = self._find_free_dev_and_pcislot(instance_name,
+                                                                used_dev)
+        if free_slot is None or free_dev is None:
+            return None
+
+        real_path = FLAGS.nbs_mountpoint_prefix + self._slot_to_disk(free_slot)
+        target_dev = '/dev/' + free_dev
+
+        return {'mountpoint': target_dev,
+                'real_path': real_path,
+                'slot': free_slot}
+
+    @exception.wrap_exception()
+    def extend_nbs_volume(self, instance_name, device, size):
+        """ """
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        mount_device = device['mountpoint'].rpartition("/")[2]
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                size_KB = size * 1024 * 1024
+                virt_dom.blockResize(mount_device, size_KB, 0)
+        except Exception as ex:
+            LOG.exception(ex)
+            raise ex
+
+    @exception.wrap_exception()
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """
+            return retry or not, if need retry, used device is returned, too.
+        """
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        mount_device = device['mountpoint'].rpartition("/")[2]
+        slot = device['slot']
+        iotune_total_bytes = qos_info['iotune_total_bytes']
+        iotune_total_iops = qos_info['iotune_total_iops']
+        disk_xml = '''<disk type='block'>
+                        <driver name='qemu' type='raw' cache='none'/>
+                        <source dev='%s'/>
+                        <target dev='%s' bus='virtio'/>
+                        <address type='pci' domain='0x0000' \
+                         bus='0x00' slot='0x%0.2x' function='0x0'/>
+                        <iotune>
+                          <total_bytes_sec>%d</total_bytes_sec>
+                          <total_iops_sec>%d</total_iops_sec>
+                        </iotune>
+                        <serial>%s</serial>
+                      </disk>''' % (host_dev, mount_device, slot,
+                                    iotune_total_bytes, iotune_total_iops,
+                                    volume_id)
+
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            # NOTE(vish): We can always affect config because our
+            #             domains are persistent, but we should only
+            #             affect live if the domain is running.
+            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+            virt_dom.attachDeviceFlags(disk_xml, flags)
+            return (False, None)
+        except Exception as ex:
+            if isinstance(ex, libvirt.libvirtError):
+                errcode = ex.get_error_code()
+                if errcode == libvirt.VIR_ERR_OPERATION_FAILED:
+                    raise exception.DeviceIsBusy(device=disk_dev)
+                elif errcode == libvirt.VIR_ERR_INVALID_ARG:
+                    # target dev already exists
+                    return (True, {'dev': mount_device})
+                elif errcode == libvirt.VIR_ERR_XML_ERROR:
+                    # pci slot already used
+                    return (True, {'slot': slot})
+                else:
+                    errmsg = ex.get_error_message()
+                    LOG.error(_("Error code %d, message %s, instance "
+                                "name %s, volume_id %s") % (errcode,
+                                    errmsg, instance_name, volume_id))
+                    raise ex
+            else:
+                raise ex
+
     @exception.wrap_exception()
     def attach_volume(self, connection_info, instance_name, mountpoint):
         virt_dom = self._lookup_by_name(instance_name)
@@ -792,6 +930,38 @@ class LibvirtDriver(driver.ComputeDriver):
         return xml
 
     @exception.wrap_exception()
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        mount_device = mountpoint.rpartition("/")[2]
+        try:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still logout even if
+            #             the instance doesn't exist here anymore.
+            virt_dom = self._lookup_by_name(instance_name)
+            xml = self._get_disk_xml(virt_dom.XMLDesc(0), mount_device)
+            if not xml:
+                raise exception.DiskNotFound(location=mount_device)
+            else:
+                flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+                state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+                if state == power_state.RUNNING:
+                    flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+                virt_dom.detachDeviceFlags(xml, flags)
+        except libvirt.libvirtError as ex:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still disconnect even if
+            #             the instance doesn't exist here anymore.
+            error_code = ex.get_error_code()
+            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
+                # NOTE(vish):
+                LOG.warn(_("During detach_volume, instance disappeared."))
+            else:
+                raise ex
+
+    @exception.wrap_exception()
     def detach_volume(self, connection_info, instance_name, mountpoint):
         mount_device = mountpoint.rpartition("/")[2]
         try:
