commit cf2fcdb49a35fa03e37384e46760c6ff344f7d8c
Author: Wangpan <hzwangpan@corp.netease.com>
Date:   Thu May 16 17:59:56 2013 +0800

    decouple nbs and nvs
    
    1. add nbs volume attachment process
    2. add nbs volume detachment process
    3. add nbs volume extension process
    4. add nbs_client.py
    5. handle block device when using nbs backend
    6. handle iotune and slot in block_device to_xml
    7. remove process detach/delete nbs volume when deleting instance
    
    Change-Id: Ia9f003f498a3748028ce89879d6eaf5685bcad94

diff --git a/etc/nova/policy.json b/etc/nova/policy.json
index 6ab4160..6f6e4ed 100644
--- a/etc/nova/policy.json
+++ b/etc/nova/policy.json
@@ -7,6 +7,8 @@
     "compute:create": [],
     "compute:create:attach_network": [],
     "compute:create:attach_volume": [],
+    "compute:create:attach_nbs_volume": [],
+    "compute:create:detach_nbs_volume": [],
     "compute:create:forced_host": [["is_admin:True"]],
     "compute:get_all": [],
 
diff --git a/nova/api/openstack/compute/contrib/server_status.py b/nova/api/openstack/compute/contrib/server_status.py
index d9368a4..175230e 100644
--- a/nova/api/openstack/compute/contrib/server_status.py
+++ b/nova/api/openstack/compute/contrib/server_status.py
@@ -14,19 +14,14 @@
 #    License for the specific language governing permissions and limitations
 #    under the License
 
-import datetime
-
-import memcache
 from webob import exc
 
 from nova.api.openstack import extensions
-from nova import db
+from nova import compute
 from nova import exception
 from nova import flags
 from nova.openstack.common import cfg
 from nova.openstack.common import log as logging
-from nova.openstack.common import timeutils
-from nova import utils
 
 
 metadata_opts = [
@@ -39,64 +34,27 @@ FLAGS = flags.FLAGS
 FLAGS.register_opts(metadata_opts)
 LOG = logging.getLogger(__name__)
 authorize = extensions.extension_authorizer('compute', 'server_status')
-MEMCACHE_CLIENT_API = None
 
 
 class ServerStatusController(object):
+    def __init__(self):
+        self.compute_api = compute.API()
 
     def show(self, req, id):
+        context = req.environ['nova.context']
+        authorize(context)
+        LOG.debug(_("Listing server status"), context=context,
+                    instance_uuid=id)
+
         try:
-            context = req.environ['nova.context']
-            authorize(context)
-            LOG.debug(_("Listing server status"))
-            result = self._instance_os_boot_ready(
-                            context, id,
-                            FLAGS.server_heartbeat_period)
+            instance = self.compute_api.get(context, id)
+            result = self.compute_api.instance_os_boot_ready(context,
+                                            instance['uuid'],
+                                            FLAGS.server_heartbeat_period)
             return result
         except exception.NotFound:
             raise exc.HTTPNotFound()
 
-    def _instance_os_boot_ready(self, context, instance_uuid,
-                                heartbeat_period):
-        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
-        status = 'down'
-        interval = heartbeat_period * 1.5
-        try:
-            db.instance_get_by_uuid(context, instance_uuid)
-        except exception.InstanceNotFound:
-            raise exception.NotFound()
-
-        cache_key = str(instance_uuid + '_heart')
-        memcache_client = self._get_memcache_client()
-        if memcache_client is not None:
-            cache_value = memcache_client.get(cache_key)
-            if cache_value:
-                last_heartbeat = datetime.datetime.strptime(
-                                        cache_value,
-                                        '%Y-%m-%d %H:%M:%S')
-                # Timestamps in DB are UTC.
-                elapsed = utils.total_seconds(
-                                timeutils.utcnow() - last_heartbeat)
-                if abs(elapsed) <= interval:
-                    status = 'up'
-        else:
-            msg = _('Store data can not catch')
-            raise exc.HTTPServerError(explanation=msg)
-
-        return {'instance_uuid': instance_uuid,
-                'status': status}
-
-    def _get_memcache_client(self):
-        """Return memcache client"""
-        global MEMCACHE_CLIENT_API
-        if MEMCACHE_CLIENT_API is not None:
-            return MEMCACHE_CLIENT_API
-        else:
-            if FLAGS.memcached_servers:
-                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
-                                                      debug=0)
-            return MEMCACHE_CLIENT_API
-
 
 class Server_status(extensions.ExtensionDescriptor):
     """Add server_status to the Create Server v1.1 API"""
diff --git a/nova/api/openstack/compute/contrib/volumes.py b/nova/api/openstack/compute/contrib/volumes.py
index 9940e30..1662ba8 100644
--- a/nova/api/openstack/compute/contrib/volumes.py
+++ b/nova/api/openstack/compute/contrib/volumes.py
@@ -15,6 +15,8 @@
 
 """The volumes extension."""
 
+import datetime
+import time
 import webob
 from webob import exc
 from xml.dom import minidom
@@ -24,9 +26,11 @@ from nova.api.openstack import extensions
 from nova.api.openstack import wsgi
 from nova.api.openstack import xmlutil
 from nova import compute
+from nova.compute import vm_states
 from nova import exception
 from nova import flags
 from nova.openstack.common import log as logging
+from nova.openstack.common import timeutils
 from nova import utils
 from nova import volume
 from nova.volume import volume_types
@@ -164,6 +168,7 @@ class VolumeController(wsgi.Controller):
     """The Volumes API controller for the OpenStack API."""
 
     def __init__(self):
+        self.compute_api = compute.API()
         self.volume_api = volume.API()
         super(VolumeController, self).__init__()
 
@@ -271,6 +276,68 @@ class VolumeController(wsgi.Controller):
 
         return wsgi.ResponseObject(result, headers=dict(location=location))
 
+    def _extend_nbs_volume(self, req, id, body):
+        """Extend a exists nbs volume."""
+        if FLAGS.nbs_api_server is None:
+            explanation = _("Cannot extend nbs volume, nbs server is None.")
+            raise exc.HTTPForbidden(explanation=explanation)
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if 'size' not in body or not isinstance(body['size'], int):
+            explanation = _("size is needed in body.")
+            raise exc.HTTPUnprocessableEntity(explanation=explanation)
+        try:
+            instance_uuid = self.compute_api.check_nbs_attached(context, id)
+        except exception.NotFound:
+            explanation = _("volume %s not found.") % id
+            raise exc.HTTPNotFound(explanation=explanation)
+        except exception.Invalid:
+            LOG.info(_("volume %s not attached.") % id)
+            instance_uuid = None
+
+        size = body['size'] # GB
+
+        if instance_uuid is not None:
+            try:
+                # check instance exists
+                instance = self.compute_api.get(context, instance_uuid)
+                # Check os status if instance is 'ACTIVE'
+                if instance['vm_state'] == vm_states.ACTIVE:
+                    server_heartbeat_period = FLAGS.get(
+                                                'server_heartbeat_period', 10)
+                    os_status = self.compute_api.instance_os_boot_ready(
+                                                    context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                    if os_status['status'] != "up":
+                        explanation = _("Cannot extend volume while instance "
+                                        "os is starting.")
+                        raise exc.HTTPForbidden(explanation=explanation)
+            except exception.MemCacheClientNotFound:
+                explanation = _("Memory cache client is not found")
+                raise exc.HTTPServerError(explanation=explanation)
+            except exception.NotFound:
+                explanation = _("Instance %s not found.") % instance_uuid
+                raise exc.HTTPNotFound(explanation=explanation)
+
+            # notify instance about this extension
+            self.compute_api.extend_nbs_volume(context, id, size, instance)
+        else:
+            # call nbs to extend this volume directly
+            self.compute_api.extend_nbs_volume(context, id, size)
+
+        return {'requestId': context.request_id, 'size': size}
+
+    # @wsgi.serializers(xml=VolumesTemplate)
+    def update(self, req, id, body):
+        """Extend a exists volume."""
+        if FLAGS.ebs_backend == 'nbs':
+            return self._extend_nbs_volume(req, id, body)
+        else:
+            raise exc.HTTPBadRequest()
+
+
 
 def _translate_attachment_detail_view(volume_id, instance_uuid, mountpoint):
     """Maps keys for attachment details view."""
@@ -375,9 +442,74 @@ class VolumeAttachmentController(wsgi.Controller):
             instance['uuid'],
             assigned_mountpoint)}
 
+    def _attach_nbs_volume(self, req, server_id, body):
+        """Attach a nbs volume to an instance."""
+        if FLAGS.nbs_api_server is None:
+            explanation = _("Cannot attach nbs volume, nbs server is None.")
+            raise exc.HTTPForbidden(explanation=explanation)
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if not self.is_valid_body(body, 'volumeAttachment'):
+            explanation = _("Paramater volumeAttachment is needed in body.")
+            raise exc.HTTPUnprocessableEntity(explanation=explanation)
+
+        volume_id = body['volumeAttachment']['volumeId']
+
+        msg = _("Attach nbs volume %s to instance %s") % (volume_id, server_id)
+        LOG.audit(msg, context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+            # TODO(wangpan): if the instance forbid to attach nbs volume,
+            #                raise exception.
+            # Check os status if instance is 'ACTIVE'
+            if instance['vm_state'] == vm_states.ACTIVE:
+                server_heartbeat_period = FLAGS.get('server_heartbeat_period',
+                                                    10)
+                os_status = self.compute_api.instance_os_boot_ready(context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                if os_status['status'] != "up":
+                    explanation = _("Cannot attach volume while instance os "
+                                    "is starting.")
+                    raise exc.HTTPForbidden(explanation=explanation)
+            device = self.compute_api.attach_nbs_volume(context, instance,
+                                                        volume_id)
+        except exception.MemCacheClientNotFound:
+            explanation = _("Memory cache client is not found")
+            raise exc.HTTPServerError(explanation=explanation)
+        except exception.NotFound:
+            explanation = _("Instance %s not found.") % server_id
+            raise exc.HTTPNotFound(explanation=explanation)
+
+        # The attach is async
+        attachment = {}
+        attachment['requestId'] = context.request_id
+        attachment['instanceId'] = instance['uuid']
+        attachment['volumeId'] = volume_id
+        attachment['device'] = device
+        attachment['status'] = "attaching"
+        attachment['attachTime'] = long(time.time())
+
+        # NOTE(justinsb): And now, we have a problem...
+        # The attach is async, so there's a window in which we don't see
+        # the attachment (until the attachment completes).  We could also
+        # get problems with concurrent requests.  I think we need an
+        # attachment state, and to write to the DB here, but that's a bigger
+        # change.
+        # For now, we'll probably have to rely on libraries being smart
+
+        # TODO(justinsb): How do I return "accepted" here?
+        return {'attachment': attachment}
+
     @wsgi.serializers(xml=VolumeAttachmentTemplate)
     def create(self, req, server_id, body):
         """Attach a volume to an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._attach_nbs_volume(req, server_id, body)
+
         context = req.environ['nova.context']
         authorize(context)
 
@@ -420,8 +552,57 @@ class VolumeAttachmentController(wsgi.Controller):
         """Update a volume attachment.  We don't currently support this."""
         raise exc.HTTPBadRequest()
 
+    def _detach_nbs_volume(self, req, server_id, id):
+        """Detach a nbs volume from an instance."""
+        if FLAGS.nbs_api_server is None:
+            explanation = _("Cannot detach nbs volume, nbs server is None.")
+            raise exc.HTTPForbidden(explanation=explanation)
+        context = req.environ['nova.context']
+        authorize(context)
+
+        volume_id = id
+        LOG.audit(_("Detach nbs volume %s from instance %s"),
+                    (volume_id, server_id), context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+        except exception.NotFound:
+            explanation = _("Instance %s not found.") % server_id
+            raise exc.HTTPNotFound(explanation=explanation)
+
+        bdms = self.compute_api.get_instance_bdms(context, instance, True)
+
+        too_short_interval = False
+        for bdm in bdms:
+            if bdm['volume_id'] == volume_id:
+                # FIXME(wangpan): we check the update time of device here
+                #                 to avoid attaching/detaching too frequently
+                #                 (vm may get exception if do that).
+                if ((timeutils.utcnow() - bdm['updated_at'])
+                    < datetime.timedelta(seconds=FLAGS.attch_detach_interval)):
+                    too_short_interval = True
+                    break
+
+        if too_short_interval:
+            explanation = _("Too short interval after attaching to detach "
+                            "volume %s." % volume_id)
+            raise exc.HTTPForbidden(explanation=explanation)
+
+        try:
+            self.compute_api.detach_nbs_volume(context,
+                    instance, volume_id)
+        except exception.Invalid:
+            explanation = _("volume %s is not attached.") % volume_id
+            raise exc.HTTPNotFound(explanation=explanation)
+        else:
+            return {'requestId': context.request_id, 'return': True}
+
     def delete(self, req, server_id, id):
         """Detach a volume from an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._detach_nbs_volume(req, server_id, id)
+
         context = req.environ['nova.context']
         authorize(context)
 
diff --git a/nova/api/openstack/compute/servers.py b/nova/api/openstack/compute/servers.py
index 45cb779..e4ec3c7 100644
--- a/nova/api/openstack/compute/servers.py
+++ b/nova/api/openstack/compute/servers.py
@@ -786,7 +786,9 @@ class Controller(wsgi.Controller):
             availability_zone = server_dict.get('availability_zone')
 
         block_device_mapping = None
-        if self.ext_mgr.is_loaded('os-volumes'):
+        # FIXME(wangpan): we currently don't support create instance with nbs
+        #                 volume block device
+        if self.ext_mgr.is_loaded('os-volumes') and FLAGS.ebs_backend != 'nbs':
             block_device_mapping = server_dict.get('block_device_mapping')
 
         ret_resv_id = False
@@ -906,12 +908,12 @@ class Controller(wsgi.Controller):
 
         return self._add_location(robj)
 
-    def _delete(self, context, req, instance_uuid, del_ebs=False):
+    def _delete(self, context, req, instance_uuid):
         instance = self._get_server(context, req, instance_uuid)
         if FLAGS.reclaim_instance_interval:
             self.compute_api.soft_delete(context, instance)
         else:
-            self.compute_api.delete(context, instance, del_ebs)
+            self.compute_api.delete(context, instance)
 
     @wsgi.serializers(xml=ServerTemplate)
     def update(self, req, id, body):
@@ -1056,21 +1058,8 @@ class Controller(wsgi.Controller):
     @wsgi.response(204)
     def delete(self, req, id):
         """Destroys a server."""
-        def convert_bool(v):
-            if isinstance(v, bool):
-                return v
-            _boolean_states = {'1': True, 'yes': True, 'true': True,
-                               'on': True, '0': False, 'no': False,
-                               'false': False, 'off': False}
-            value = _boolean_states.get(v.lower())
-            if value is None:
-                LOG.error(_("invalid value of X-Del-Ebs header(%s)"), v)
-                return False
-            return value
-
         try:
-            del_ebs = convert_bool(req.headers.get("x-del-ebs", False))
-            self._delete(req.environ['nova.context'], req, id, del_ebs)
+            self._delete(req.environ['nova.context'], req, id)
         except exception.NotFound:
             raise exc.HTTPNotFound()
         except exception.InstanceInvalidState as state_error:
diff --git a/nova/compute/api.py b/nova/compute/api.py
index bd417fe..65a92be 100644
--- a/nova/compute/api.py
+++ b/nova/compute/api.py
@@ -22,14 +22,18 @@
 networking and storage of VMs, and compute hosts on which they run)."""
 
 import base64
+import datetime
 import functools
 import re
 import string
 import time
 import urllib
 
+import memcache
+
 from nova import block_device
 from nova.compute import instance_types
+from nova.compute import nbs_client
 from nova.compute import power_state
 from nova.compute import rpcapi as compute_rpcapi
 from nova.compute import task_states
@@ -63,6 +67,7 @@ flags.DECLARE('consoleauth_topic', 'nova.consoleauth')
 
 MAX_USERDATA_SIZE = 65535
 QUOTAS = quota.QUOTAS
+MEMCACHE_CLIENT_API = None
 
 
 def check_instance_state(vm_state=None, task_state=(None,)):
@@ -146,6 +151,7 @@ class API(base.Base):
 
         self.network_api = network_api or network.API()
         self.volume_api = volume_api or volume.API()
+        self.nbs_api = nbs_client.API()
         self.security_group_api = security_group_api or SecurityGroupAPI()
         self.sgh = importutils.import_object(FLAGS.security_group_handler)
         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
@@ -951,10 +957,15 @@ class API(base.Base):
                 # NOTE(comstud): Race condition.  Instance already gone.
                 pass
 
-    def _delete(self, context, instance, del_ebs=False):
+    def _delete(self, context, instance):
         host = instance['host']
-        bdms = self.db.block_device_mapping_get_all_by_instance(
-            context, instance['uuid'])
+        if FLAGS.ebs_backend == 'nbs':
+            # FIXME(wangpan): we ask users to handle nbs volumes
+            #                 before deleting
+            bdms = []
+        else:
+            bdms = self.db.block_device_mapping_get_all_by_instance(
+                context, instance['uuid'])
         reservations = None
 
         if context.is_admin and context.project_id != instance['project_id']:
@@ -1038,8 +1049,7 @@ class API(base.Base):
             for service in services:
                 if utils.service_is_up(service):
                     is_up = True
-                    self.compute_rpcapi.terminate_instance(context,
-                                                        instance, del_ebs)
+                    self.compute_rpcapi.terminate_instance(context, instance)
                     break
             if not is_up:
                 # If compute node isn't up, just delete from DB
@@ -1149,14 +1159,14 @@ class API(base.Base):
     @wrap_check_policy
     @check_instance_lock
     @check_instance_state(vm_state=None, task_state=None)
-    def delete(self, context, instance, del_ebs=False):
+    def delete(self, context, instance):
         """Terminate an instance."""
         LOG.debug(_("Going to try to terminate instance"), instance=instance)
 
         if instance['disable_terminate']:
             return
 
-        self._delete(context, instance, del_ebs)
+        self._delete(context, instance)
 
     @wrap_check_policy
     @check_instance_lock
@@ -2104,6 +2114,138 @@ class API(base.Base):
         """Inject network info for the instance."""
         self.compute_rpcapi.inject_network_info(context, instance=instance)
 
+    def _get_memcache_client(self):
+        """Return memcache client."""
+        global MEMCACHE_CLIENT_API
+        if MEMCACHE_CLIENT_API is not None:
+            return MEMCACHE_CLIENT_API
+        else:
+            if FLAGS.memcached_servers:
+                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
+                                                      debug=0)
+            return MEMCACHE_CLIENT_API
+
+    def instance_os_boot_ready(self, context, instance_uuid, heartbeat_period):
+        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
+        status = 'down'
+        interval = heartbeat_period * 1.5
+
+        cache_key = str(instance_uuid + '_heart')
+        memcache_client = self._get_memcache_client()
+        if memcache_client is not None:
+            cache_value = memcache_client.get(cache_key)
+            if cache_value:
+                last_heartbeat = datetime.datetime.strptime(
+                                        cache_value,
+                                        '%Y-%m-%d %H:%M:%S')
+                # Timestamps in DB are UTC.
+                elapsed = utils.total_seconds(
+                                timeutils.utcnow() - last_heartbeat)
+                if abs(elapsed) <= interval:
+                    status = 'up'
+        else:
+            raise exception.MemCacheClientNotFound()
+
+        return {'instance_uuid': instance_uuid,
+                'status': status}
+
+    def check_nbs_attached(self, context, volume_id, instance_uuid=None):
+        """Check nbs volume is attached or not."""
+        # FIXME(wangpan): we just deal with single attachment status now
+        try:
+            volume = self.nbs_api.get(context, volume_id)['volumes'][0]
+        except (IndexError, KeyError, TypeError):
+            raise exception.VolumeNotFound(volume_id=volume_id)
+
+        try:
+            attachment = volume['attachments'][0]
+            if attachment['status'] not in ('attachedInVM', 'attached'):
+                msg = _("Volume must be attached in order to detach.")
+                raise exception.InvalidVolume(reason=msg)
+        except (IndexError, KeyError, TypeError):
+            msg = _("Volume must be attached in order to detach.")
+            raise exception.InvalidVolume(reason=msg)
+
+        try:
+            if (instance_uuid is not None and
+                    attachment['instanceId'] != instance_uuid):
+                raise exception.VolumeUnattached(volume_id=volume_id)
+        except (KeyError, TypeError):
+            raise exception.VolumeUnattached(volume_id=volume_id)
+
+        if attachment['status'] == 'attachedInVM':
+            return attachment.get('instanceId', None)
+        else:
+            return None
+
+    def _call_nbs_to_extend(self, context, volume_id, size):
+        self.nbs_api.extend(context, volume_id, size)
+
+    @check_instance_lock
+    def _extend_nbs_volume(self, context, instance, volume_id, size):
+        self.compute_rpcapi.extend_nbs_volume(context, instance=instance,
+                        volume_id=volume_id, size=size, device=None)
+
+    def extend_nbs_volume(self, context, volume_id, size, instance=None):
+        """
+        Extend nbs volume, and if it has been attached, call libvirt to
+        extend it in instance.
+        """
+        if instance is None:
+            # call nbs to extend the volume, we do nothing
+            self._call_nbs_to_extend(context, volume_id, size)
+        else:
+            self._extend_nbs_volume(context, instance, volume_id, size)
+
+    @wrap_check_policy
+    @check_instance_lock
+    def attach_nbs_volume(self, context, instance, volume_id):
+        """Attach an existing volume to an existing instance."""
+        # NOTE(vish): This is done on the compute host because we want
+        #             to avoid a race where two devices are requested at
+        #             the same time. When db access is removed from
+        #             compute, the bdm will be created here and we will
+        #             have to make sure that they are assigned atomically.
+
+        # Check volume exists and is available to attach
+        volumes = self.nbs_api.get(context, volume_id)['volumes']
+        if len(volumes) == 0:
+            raise exception.VolumeNotFound(volume_id=volume_id)
+
+        volume = volumes[0]
+        if volume.get('status', None) != 'available':
+            reason = _("Volume %s is not available") % volume_id
+            raise exception.InvalidVolume(reason=reason)
+
+        device = self.compute_rpcapi.get_device_for_nbs_volume(
+                        context, instance=instance)
+        if device is None:
+            # can't find a slot or target dev to attach
+            LOG.error(_("Cannot find a free slot or target dev to attach "
+                        "volume %s") % volume_id,
+                        context=context, instance=instance)
+            raise exception.NoFreeDevice()
+
+        # deal with race condition here,
+        # such as attaching a volume two times at a short interval
+        db_vol = self.db.block_device_mapping_get_all_by_volume(context,
+                                                                volume_id)
+        if len(db_vol) > 0:
+            self.db.block_device_mapping_destroy_by_instance_and_device(
+                        context, instance['uuid'], device)
+            reason = _("Volume %s is in use") % volume_id
+            raise exception.InvalidVolume(reason=reason)
+        else:
+            values = {'instance_uuid': instance['uuid'],
+                      'virtual_name': None,
+                      'device_name': jsonutils.dumps(device),
+                      'volume_id': volume_id}
+            self.db.block_device_mapping_update_or_create(context, values)
+        self.compute_rpcapi.attach_nbs_volume(context, instance=instance,
+                volume_id=volume_id, device=device)
+
+        return device['real_path']
+
     @wrap_check_policy
     @check_instance_lock
     def attach_volume(self, context, instance, volume_id, device=None):
@@ -2135,6 +2277,15 @@ class API(base.Base):
         return device
 
     @check_instance_lock
+    def detach_nbs_volume(self, context, instance, volume_id):
+        """Detach a volume from an instance."""
+        self.check_nbs_attached(context, volume_id, instance['uuid'])
+
+        check_policy(context, 'detach_nbs_volume', instance)
+        self.compute_rpcapi.detach_nbs_volume(context, instance=instance,
+                volume_id=volume_id)
+
+    @check_instance_lock
     def _detach_volume(self, context, instance, volume_id):
         check_policy(context, 'detach_volume', instance)
 
@@ -2219,8 +2370,12 @@ class API(base.Base):
         uuids = [instance['uuid'] for instance in instances]
         return self.db.instance_fault_get_by_instance_uuids(context, uuids)
 
-    def get_instance_bdms(self, context, instance):
+    def get_instance_bdms(self, context, instance, is_nbs=False):
         """Get all bdm tables for specified instance."""
+        # FIXME(wangpan): we currently don't deal with nbs volume at most
+        #                 callers, except the caller expects to get nbs volume
+        if FLAGS.ebs_backend == 'nbs' and not is_nbs:
+            return []
         return self.db.block_device_mapping_get_all_by_instance(context,
                 instance['uuid'])
 
diff --git a/nova/compute/ebs_client.py b/nova/compute/ebs_client.py
deleted file mode 100644
index 24de7ef..0000000
--- a/nova/compute/ebs_client.py
+++ /dev/null
@@ -1,210 +0,0 @@
-# author: hzwangpan@corp.netease.com
-# client of EBS API for nova to check and detach all volume(s) when VM is
-# going to shutdown, make sure the volume status in EBS is identical with
-# the status in nova.
-
-import httplib
-import json
-import time
-
-from nova import flags
-from nova.openstack.common import cfg
-from nova.openstack.common import log as logging
-from nova.openstack.common.notifier import api as notifier_api
-
-ebs_opts = [
-    cfg.StrOpt('ebs_api_server',
-            default=None,
-            help='the host and port of ebs server'),
-    cfg.StrOpt('ebs_prefix_url',
-            default='EBS',
-            help='the url prefix of ebs server, the final url should like '
-                 'this: ebs_api_server/ebs_prefix_url/?Action=XX&YY=ZZ...')
-]
-
-FLAGS = flags.FLAGS
-FLAGS.register_opts(ebs_opts)
-
-LOG = logging.getLogger(__name__)
-
-
-class API():
-    def __init__(self):
-        self._ebs_api_server = FLAGS.ebs_api_server or ""
-        if "http://" in self._ebs_api_server:
-            self._ebs_api_server = self._ebs_api_server.replace("http://", "")
-        if "/" in self._ebs_api_server:
-            self._ebs_api_server = self._ebs_api_server.replace("/", "")
-
-    def get(self, context, instance_uuid=None, volume_id=None):
-        if not instance_uuid and not volume_id:
-            LOG.error(_("at least one argument between instance_uuid and "
-                        "volume_id should be given"), context=context)
-            return None
-
-        project_id = context.project_id
-        url = "/" + FLAGS.ebs_prefix_url + "/?Action=DescribeVolumes" \
-              + "&ProjectId=" + str(project_id)
-        if instance_uuid:
-            url = url + "&InstanceId=" + str(instance_uuid)
-        if volume_id:
-            url = url + "&VolumeId=" + str(volume_id)
-        headers = {"Content-type": "application/json",
-                   "Accept": "application/json"}
-        params = None
-        method = "GET"
-        return self._request(context, method, url, params, headers)
-
-    def _detach(self, context, volume_id):
-        if not volume_id:
-            LOG.warn(_("volume id is None, do nothing and return now"),
-                        context=context)
-            return None
-
-        project_id = context.project_id
-        url = "/" + FLAGS.ebs_prefix_url + "/?Action=DetachVolume" \
-              + "&ProjectId=" + str(project_id) \
-              + "&VolumeId=" + str(volume_id)
-        headers = {"Content-type": "application/json",
-                   "Accept": "application/json"}
-        params = None
-        method = "GET"
-        return self._request(context, method, url, params, headers)
-
-    def detach_all(self, context, volumes):
-        volume_ids = [vol.get("volumeId") for vol in volumes["volumes"]]
-        for volume_id in volume_ids:
-            self._detach(context, volume_id)
-
-    def _delete(self, context, volume_id):
-        vol_detail = self.get(context, volume_id=volume_id)
-        if not vol_detail or "volumes" not in vol_detail:
-            LOG.warn(_("volume %s not found") % str(volume_id),
-                       context=context)
-            return
-        vol = vol_detail["volumes"][0]
-        if vol.get("status") != "available" and vol.get("status") != "error":
-            LOG.warn(_("volume %s status is NOT available or error, will not "
-                        "be deleted") % str(volume_id), context=context)
-            return
-
-        project_id = context.project_id
-        url = "/" + FLAGS.ebs_prefix_url + "/?Action=DeleteVolume" \
-              + "&ProjectId=" + str(project_id) \
-              + "&VolumeId=" + str(volume_id)
-        headers = {"Content-type": "application/json",
-                   "Accept": "application/json"}
-        params = None
-        method = "GET"
-        self._request(context, method, url, params, headers)
-
-    def delete_all(self, context, volumes):
-        volume_ids = [vol.get("volumeId") for vol in volumes["volumes"]]
-        for volume_id in volume_ids:
-            self._delete(context, volume_id)
-
-    def wait_for_detach(self, context, instance_uuid,
-                        check_interval=3, check_times=10):
-        # FIXME:How to deal with the timeout situation?
-        for times in range(check_times):
-            all_detached = True
-            volume_details = self.get(context, instance_uuid=instance_uuid)
-            # if we get nothing or wrong content from the EBS server,
-            # we don't retry and return immediately.
-            if not volume_details or "volumes" not in volume_details:
-                LOG.warn(_("get nothing from ebs server, return now, "
-                           "times: %d") % (times + 1), context=context)
-                return False
-            for vol in volume_details["volumes"]:
-                if vol.get("status") != "available" \
-                            and vol.get("status") != "error":
-                    all_detached = False
-                    break
-            if all_detached:
-                return True
-            LOG.info(_("sleep %(check_interval)ds and retry to check volumes'"
-                   " status, times:%(times)d") % locals(), context=context)
-            if times != check_times - 1:
-                time.sleep(check_interval)
-        LOG.warn(_("NOT all volumes are detached successfully after %ds") %
-                    (check_interval * check_times), context=context)
-        return False
-
-    def _request(self, context, method, url, params, headers):
-        if not self._ebs_api_server:
-            LOG.warn(_("ebs_api_server is null, can't connect to it"))
-            return None
-
-        unified_log_id = None
-        unified_log_seq = None
-        if context:
-            unified_log_id = context.to_dict().get('unified_log_id', None)
-            unified_log_seq = context.to_dict().get('unified_log_seq', None)
-        if unified_log_id and unified_log_seq:
-            log_seq_nums = unified_log_seq.split('.')
-            log_seq_nums[-1] = str(int(log_seq_nums[-1]) + 1)
-            new_log_seq = '.'.join(log_seq_nums)
-            context.unified_log_seq = new_log_seq
-
-            url = url + "&LogId=" + str(unified_log_id)
-            url = url + "&LogSeq=" + str(new_log_seq)
-
-        full_url = self._ebs_api_server + url
-
-        LOG.info(_("send request to %(full_url)s, method: %(method)s, "
-                   "body: %(params)s, headers: %(headers)s") % locals(),
-                   context=context)
-        try:
-            ebs_conn = httplib.HTTPConnection(self._ebs_api_server)
-        except Exception, ex:
-            LOG.error(_("exception occurs when connect to ebs server, "
-                        "error msg: %s") % str(ex), context=context)
-            return None
-
-        try:
-            ebs_conn.request(method, url, params, headers)
-            resp = ebs_conn.getresponse()
-            if resp:
-                if resp.status == 200:
-                    data = json.loads(resp.read())
-                    request_id = data["requestId"]
-                    LOG.info(_("request id: %(request_id)s, data: "
-                               "%(data)s") % locals(), context=context)
-                    return data
-                else:
-                    err_code = resp.status
-                    err_reason = resp.reason
-                    LOG.error(_("error occurs when contact to ebs server, "
-                                "error code: %(err_code)d, "
-                                "reason: %(err_reason)s") % locals(),
-                                context=context)
-                    self._notify_NBS_connection_failure(context, full_url)
-                    return None
-            else:
-                LOG.error(_("ebs server doesn't return any response"),
-                            context=context)
-                self._notify_NBS_connection_failure(context, full_url)
-                return None
-
-        except Exception, ex:
-            LOG.error(_("exception occurs when send request to ebs server, "
-                        "error msg: %s") % str(ex), context=context)
-            self._notify_NBS_connection_failure(context, full_url)
-            return None
-        finally:
-            ebs_conn.close()
-
-    def _notify_NBS_connection_failure(self, context, url):
-        """
-        Send a message to notification about NBS connection failure.
-        """
-        try:
-            LOG.info(_('notify NBS connection failure'))
-            payload = dict({'url': url})
-            notifier_api.notify(context,
-                                notifier_api.publisher_id('api_nbs'),
-                                'api_nbs.nvs_connect_nbs_failure',
-                                notifier_api.ERROR, payload)
-        except Exception:
-            LOG.exception(_('notification module error when do notifying '
-                            'NVS connect NBS failed.'))
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index e863e73..8483771 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -36,12 +36,14 @@ terminating it.
 
 import contextlib
 import copy
+import fcntl
 import functools
 import json
 from operator import itemgetter
 import os
 import re
 import socket
+import struct
 import sys
 import time
 import traceback
@@ -50,8 +52,8 @@ from eventlet import greenthread
 
 from nova import block_device
 from nova import compute
-from nova.compute import ebs_client
 from nova.compute import instance_types
+from nova.compute import nbs_client
 from nova.compute import power_state
 from nova.compute import resource_tracker
 from nova.compute import rpcapi as compute_rpcapi
@@ -356,7 +358,7 @@ class ComputeManager(manager.SchedulerDependentManager):
 
         self.network_api = network.API()
         self.volume_api = volume.API()
-        self.ebs_api = ebs_client.API()
+        self.nbs_api = nbs_client.API()
         self._last_host_check = 0
         self._last_bw_usage_poll = 0
         self._last_info_cache_heal = 0
@@ -466,6 +468,34 @@ class ComputeManager(manager.SchedulerDependentManager):
         else:
             LOG.warning(_('get instance storage device for ioqos failed!'))
 
+    def _check_and_wait_nbs_vol(self, context, instance_uuid, wait_ticks):
+        """
+        Check if nbs volumes disappeared, if that wait for nbs volumes appear.
+        """
+        bdms = self._get_instance_volume_bdms(context, instance_uuid, True)
+        disappeared_host_devs = []
+
+        for bdm in bdms:
+            host_dev = jsonutils.loads(bdm['connect_info']).get('host_dev')
+            if host_dev is not None and not os.path.exists(host_dev):
+                disappeared_host_devs.append(host_dev)
+
+        if len(disappeared_host_devs) == 0:
+            return wait_ticks
+
+        while len(disappeared_host_devs) > 0 and wait_ticks > 0:
+            LOG.info(_("Wait for nbs volume %s 3 seconds") % host_dev)
+            time.sleep(3)
+            wait_ticks -= 1
+            disappeared_host_devs = [host_dev for host_dev in
+                        disappeared_host_devs if not os.path.exists(host_dev)]
+
+        if len(disappeared_host_devs) > 0:
+            LOG.warning(_("Nbs volume disappeared %s which is used by %s")
+                        % (disappeared_host_devs, instance_uuid))
+
+        return wait_ticks
+
     def init_host(self):
         """Initialization for a standalone compute service."""
         self.driver.init_host(host=self.host)
@@ -478,6 +508,8 @@ class ComputeManager(manager.SchedulerDependentManager):
         if FLAGS.defer_iptables_apply:
             self.driver.filter_defer_apply_on()
 
+        # FIXME(wangpan): sticked in 3s here.
+        wait_ticks = FLAGS.nbs_boot_wait_timeout / 3
         try:
             for count, instance in enumerate(instances):
                 db_state = instance['power_state']
@@ -514,9 +546,12 @@ class ComputeManager(manager.SchedulerDependentManager):
                            _('Rebooting instance after nova-compute restart.'),
                            locals(), instance=instance)
 
+                    wait_ticks = self._check_and_wait_nbs_vol(context,
+                                                instance['uuid'], wait_ticks)
+
                     block_device_info = \
                         self._get_instance_volume_block_device_info(
-                            context, instance['uuid'])
+                            context, instance['uuid'], True)
 
                     try:
                         self.driver.resume_state_on_host_boot(
@@ -624,6 +659,11 @@ class ComputeManager(manager.SchedulerDependentManager):
 
     def _setup_block_device_mapping(self, context, instance):
         """setup volumes for block device mapping"""
+        # FIXME(wangpan): currently we don't support create
+        #                 instance with block device
+        if FLAGS.ebs_backend == 'nbs':
+            return None
+
         block_device_mapping = []
         swap = None
         ephemerals = []
@@ -1368,13 +1408,19 @@ class ComputeManager(manager.SchedulerDependentManager):
         LOG.debug(_('Deallocating network for instance'), instance=instance)
         self.network_api.deallocate_for_instance(context, instance)
 
-    def _get_instance_volume_bdms(self, context, instance_uuid):
-        bdms = self.db.block_device_mapping_get_all_by_instance(context,
-                                                                instance_uuid)
+    def _get_instance_volume_bdms(self, context, instance_uuid, is_nbs=False):
+        # FIXME(wangpan): we currently don't deal with nbs volume at most
+        #                 callers, except the caller expects to get nbs volume
+        if FLAGS.ebs_backend == 'nbs' and not is_nbs:
+            bdms = []
+        else:
+            bdms = self.db.block_device_mapping_get_all_by_instance(context,
+                                                        instance_uuid)
         return [bdm for bdm in bdms if bdm['volume_id']]
 
-    def _get_instance_volume_bdm(self, context, instance_uuid, volume_id):
-        bdms = self._get_instance_volume_bdms(context, instance_uuid)
+    def _get_instance_volume_bdm(self, context, instance_uuid, volume_id,
+                                is_nbs=False):
+        bdms = self._get_instance_volume_bdms(context, instance_uuid, is_nbs)
         for bdm in bdms:
             # NOTE(vish): Comparing as strings because the os_api doesn't
             #             convert to integer and we may wish to support uuids
@@ -1382,9 +1428,27 @@ class ComputeManager(manager.SchedulerDependentManager):
             if str(bdm['volume_id']) == str(volume_id):
                 return bdm
 
-    def _get_instance_volume_block_device_info(self, context, instance_uuid):
-        bdms = self._get_instance_volume_bdms(context, instance_uuid)
+    def _get_instance_volume_block_device_info(self, context, instance_uuid,
+            is_nbs=False):
+        bdms = self._get_instance_volume_bdms(context, instance_uuid, is_nbs)
         block_device_mapping = []
+        # FIXME(wangpan): we need to prep nbs volumes automatically during
+        #                 all migrations such as resize, migrate, live
+        #                 migration, relocate...
+        #                 but currently we don't do this...
+        if FLAGS.ebs_backend == 'nbs' and is_nbs:
+            for bdm in bdms:
+                cinfo = jsonutils.loads(bdm['connection_info'])
+                device = jsonutils.loads(bdm['device_name'])
+                bdmap = {'host_dev': cinfo.get('host_dev', None),
+                         'qos_info': cinfo.get('qos_info', None),
+                         'mount_device': device.get('mountpoint', None),
+                         'slot': device.get('slot', None),
+                         'serial': bdm.get('volume_id', None)}
+                block_device_mapping.append(bdmap)
+
+            return {'block_device_mapping': block_device_mapping}
+
         for bdm in bdms:
             try:
                 cinfo = jsonutils.loads(bdm['connection_info'])
@@ -1422,7 +1486,7 @@ class ComputeManager(manager.SchedulerDependentManager):
                     admin_password, is_first_time, instance)
         do_run_instance()
 
-    def _shutdown_instance(self, context, instance, del_ebs=False):
+    def _shutdown_instance(self, context, instance):
         """Shutdown an instance on this host."""
         context = context.elevated()
         LOG.audit(_('%(action_str)s instance') % {'action_str': 'Terminating'},
@@ -1442,6 +1506,8 @@ class ComputeManager(manager.SchedulerDependentManager):
                   instance=instance)
 
         # NOTE(vish) get bdms before destroying the instance
+        # FIXME(wangpan): we get nbs volumes from nbs server, and detach them
+        #                 by using nbs client
         bdms = self._get_instance_volume_bdms(context, instance['uuid'])
         block_device_info = self._get_instance_volume_block_device_info(
             context, instance['uuid'])
@@ -1452,21 +1518,6 @@ class ComputeManager(manager.SchedulerDependentManager):
         LOG.debug(_("libvirt driver destroy instance end."),
                   context=context, instance=instance)
 
-        # detach all ebs volumes when terminate instance,
-        # if 'x-del-ebs' header is True, volumes will be deleted
-        ebs_volumes = self.ebs_api.get(context, instance['uuid'])
-        if ebs_volumes and "volumes" in ebs_volumes \
-                      and ebs_volumes.get("volumes") != []:
-            LOG.debug(_("find volumes %s attached, detach all now"),
-                    ebs_volumes["volumes"], instance=instance, context=context)
-            self.ebs_api.detach_all(context, ebs_volumes)
-            self.ebs_api.wait_for_detach(context, instance['uuid'])
-            if del_ebs:
-                LOG.debug(_("delete all attached ebs volumes now"),
-                            instance=instance, context=context)
-                self.ebs_api.delete_all(context, ebs_volumes)
-        # add end
-
         for bdm in bdms:
             try:
                 # NOTE(vish): actual driver detach done in driver.destroy, so
@@ -1487,6 +1538,11 @@ class ComputeManager(manager.SchedulerDependentManager):
         self._notify_about_instance_usage(context, instance, "shutdown.end")
 
     def _cleanup_volumes(self, context, instance_uuid):
+        # FIXME(wangpan): we get nbs volumes from nbs server, and detach them
+        #                 by using nbs client
+        if FLAGS.ebs_backend == 'nbs':
+            return
+
         bdms = self.db.block_device_mapping_get_all_by_instance(context,
                                                                 instance_uuid)
         for bdm in bdms:
@@ -1497,12 +1553,12 @@ class ComputeManager(manager.SchedulerDependentManager):
                 self.volume_api.delete(context, volume)
             # NOTE(vish): bdms will be deleted on instance destroy
 
-    def _delete_instance(self, context, instance, del_ebs=False):
+    def _delete_instance(self, context, instance):
         """Delete an instance on this host."""
         instance_uuid = instance['uuid']
         self.db.instance_info_cache_delete(context, instance_uuid)
         self._notify_about_instance_usage(context, instance, "delete.start")
-        self._shutdown_instance(context, instance, del_ebs)
+        self._shutdown_instance(context, instance)
         # NOTE(vish): We have already deleted the instance, so we have
         #             to ignore problems cleaning up the volumes. It would
         #             be nice to let the user know somehow that the volume
@@ -1531,7 +1587,7 @@ class ComputeManager(manager.SchedulerDependentManager):
 
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @wrap_instance_fault
-    def terminate_instance(self, context, instance, del_ebs=False):
+    def terminate_instance(self, context, instance):
         """Terminate an instance on this host.  """
         # Note(eglynn): we do not decorate this action with reverts_task_state
         # because a failure during termination should leave the task state as
@@ -1542,9 +1598,9 @@ class ComputeManager(manager.SchedulerDependentManager):
         elevated = context.elevated()
 
         @utils.synchronized(instance['uuid'])
-        def do_terminate_instance(instance, del_ebs=False):
+        def do_terminate_instance(instance):
             try:
-                self._delete_instance(context, instance, del_ebs)
+                self._delete_instance(context, instance)
             except exception.InstanceTerminationFailure as error:
                 msg = _('%s. Setting instance vm_state to ERROR')
                 LOG.error(msg % error, instance=instance)
@@ -1552,7 +1608,7 @@ class ComputeManager(manager.SchedulerDependentManager):
             except exception.InstanceNotFound as e:
                 LOG.warn(e, instance=instance)
 
-        do_terminate_instance(instance, del_ebs)
+        do_terminate_instance(instance)
 
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
@@ -1724,7 +1780,7 @@ class ComputeManager(manager.SchedulerDependentManager):
         network_info = self._get_instance_nw_info(context, instance)
 
         block_device_info = self._get_instance_volume_block_device_info(
-                            context, instance['uuid'])
+                            context, instance['uuid'], True)
 
         try:
             self.driver.reboot(instance, self._legacy_nw_info(network_info),
@@ -2500,7 +2556,12 @@ class ComputeManager(manager.SchedulerDependentManager):
                                 self._legacy_nw_info(network_info),
                                 block_device_info)
 
-            bdms = self._get_instance_volume_bdms(context, instance['uuid'])
+            # FIXME(wangpan): we need to prep nbs volumes automatically during
+            #                 all migrations such as resize, migrate, live
+            #                 migration, relocate...
+            #                 but currently we don't do this...
+            bdms = self._get_instance_volume_bdms(context,
+                        instance['uuid'])
             if bdms:
                 connector = self.driver.get_volume_connector(instance)
                 for bdm in bdms:
@@ -2594,9 +2655,14 @@ class ComputeManager(manager.SchedulerDependentManager):
             else:
                 image_meta = self._check_image_size(context, instance)
 
+            # FIXME(wangpan): we need to prep nbs volumes automatically during
+            #                 all migrations such as resize, migrate, live
+            #                 migration, relocate...
+            #                 but currently we don't do this...
             # Note(gtt): We need to extract initialize_connection into a
             # method. We do the logical all over the world.
-            bdms = self._get_instance_volume_bdms(context, instance['uuid'])
+            bdms = self._get_instance_volume_bdms(context,
+                                                instance['uuid'])
 
             if bdms:
                 connector = self.driver.get_volume_connector(instance)
@@ -2878,6 +2944,23 @@ class ComputeManager(manager.SchedulerDependentManager):
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
+    def get_device_for_nbs_volume(self, context, instance, used_dev=None):
+
+        @utils.synchronized(instance['uuid'])
+        def do_reserve():
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(result)}
+            self.db.block_device_mapping_create(context, values)
+            return result
+        return do_reserve()
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
     def reserve_block_device_name(self, context, instance, device):
 
         @utils.synchronized(instance['uuid'])
@@ -2892,6 +2975,178 @@ class ComputeManager(manager.SchedulerDependentManager):
             return result
         return do_reserve()
 
+    def _get_host_ip_by_ifname(self, ifname):
+        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        try:
+            ip = socket.inet_ntoa(fcntl.ioctl(
+                                              s.fileno(),
+                                              0x8915,  # SIOCGIFADDR
+                                              struct.pack('256s', ifname[:15])
+                                              )[20:24])
+            return ip
+        except Exception:
+            return None
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def extend_nbs_volume(self, context, instance, volume_id, size, device):
+        """Extend a nbs volume which has been attached on an instance."""
+        LOG.audit(_('Extending nbs volume %s') % volume_id,
+                    context=context, instance=instance)
+        # Get attached device from nova db
+        if device is None:
+            bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                                volume_id, True)
+            device = jsonutils.loads(bdm['device_name'])
+
+        # call nbs to extend this volume firstly
+        host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+        result = self.nbs_api.extend(context, volume_id, size, host_ip)
+        if result is None:
+            raise exception.NbsException()
+
+        # wait for nbs extending finish
+        succ = self.nbs_api.wait_for_extended(context, volume_id, size)
+        if not succ:
+            raise exception.NbsTimeout()
+
+        try:
+            return self.driver.extend_nbs_volume(instance['name'],
+                                                device, size)
+        except Exception:
+            LOG.warn(_("Extend nbs volume %s failed") % volume_id,
+                        context=context, instance=instance)
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def attach_nbs_volume(self, context, volume_id, device, instance):
+        """Attach a nbs volume to an instance."""
+        # TODO(wangpan): if this host is forbidden to attach nbs volume,
+        #                an exception needs to be raised.
+        try:
+            return self._attach_nbs_volume(context, volume_id,
+                                           device, instance)
+        except Exception:
+            with excutils.save_and_reraise_exception():
+                self.db.block_device_mapping_destroy_by_instance_and_device(
+                                                context, instance.get('uuid'),
+                                                jsonutils.dumps(device))
+
+    def _attach_nbs_volume(self, context, volume_id, device, instance):
+        # context = context.elevated()
+        LOG.audit(_('Attaching nbs volume %(volume_id)s to %(device)s'),
+                  locals(), context=context, instance=instance)
+        try:
+            # call nbs to attach volume to this host
+            host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+            result = self.nbs_api.attach(context, volume_id, instance['uuid'],
+                                        host_ip, device['real_path'])
+            if result is None:
+                raise exception.NbsException()
+
+            # check volume status, wait for nbs attaching finish
+            succ = self.nbs_api.wait_for_attached(context, volume_id,
+                                                    instance['uuid'])
+            if not succ:
+                raise exception.NbsTimeout()
+
+            # get host dev path and QoS params from nbs
+            result = self.nbs_api.get_host_dev_and_qos_info(context, volume_id,
+                                                            host_ip)
+            if result is None:
+                raise exception.NbsException()
+
+            host_dev = result.get('devicePath', None)
+            if host_dev is None:
+                raise exception.NbsException()
+            iotune_total_bytes = (None if result.get('maxBandWith', None)
+                                    is None else int(result['maxBandWith']))
+            iotune_total_iops = (None if result.get('maxIOPS', None)
+                                    is None else int(result['maxIOPS']))
+            qos_info = {'iotune_total_bytes': iotune_total_bytes,
+                        'iotune_total_iops': iotune_total_bytes}
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to connect to volume %(volume_id)s "
+                        "while attaching at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+                self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                        'attach', False,
+                                                        device['real_path'],
+                                                        host_ip,
+                                                        instance['uuid'])
+
+        @utils.synchronized(instance['uuid'])
+        def get_new_device(used_dev):
+            """
+            We get a new free slot and target dev by calling libvirt driver.
+            """
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(result),
+                      'virtual_name': None}
+            self.db.block_device_mapping_update_or_create(context, values)
+            return result
+
+        try:
+            used_dev = {'dev': [], 'slot': []}
+            retry = True
+            while retry:
+                retry, used = self.driver.attach_nbs_volume(instance['name'],
+                                    device, host_dev, qos_info, volume_id)
+                if retry:
+                    if used['dev']:
+                        used_dev['dev'].append(used['dev'])
+                    elif used['slot']:
+                        used_dev['slot'].append(used['slot'])
+                    device = get_new_device(used_dev)
+                    if device is None:
+                        # can't find a slot or target dev to attach
+                        LOG.error(_("Cannot find a free slot or target dev "
+                                    "to attach volume %s") % volume_id,
+                                    context=context, instance=instance)
+                        raise exception.NoFreeDevice()
+                    LOG.info(_("Retry to attach nbs volume %s at %s"
+                            % (volume_id, device)),
+                            context=context, instance=instance)
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to attach nbs volume %(volume_id)s "
+                        "at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+                self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                                        'attach', False,
+                                                        device['real_path'],
+                                                        host_ip,
+                                                        instance['uuid'])
+        else:
+            # notify nbs attach successfully
+            self.nbs_api.notify_nbs_libvirt_result(context, volume_id,
+                                    'attach', True, device['real_path'],
+                                    host_ip, instance['uuid'])
+
+        values = {
+            'instance_uuid': instance['uuid'],
+            'connection_info': jsonutils.dumps({'host_dev': host_dev,
+                                                'qos_info': qos_info}),
+            'device_name': jsonutils.dumps(device),
+            'delete_on_termination': False,
+            'virtual_name': None,
+            'snapshot_id': None,
+            'volume_id': volume_id,
+            'volume_size': None,
+            'no_device': None}
+        self.db.block_device_mapping_update_or_create(context, values)
+
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
@@ -2956,6 +3211,52 @@ class ComputeManager(manager.SchedulerDependentManager):
             'no_device': None}
         self.db.block_device_mapping_update_or_create(context, values)
 
+    def _detach_nbs_volume(self, context, instance, bdm):
+        """Do the actual driver detach using block device mapping."""
+        mp = jsonutils.loads(bdm['device_name'])['mountpoint']
+        volume_id = bdm['volume_id']
+
+        LOG.audit(_('Detach nbs volume %(volume_id)s from mountpoint %(mp)s'),
+                  locals(), context=context, instance=instance)
+
+        try:
+            self.driver.detach_nbs_volume(instance['name'], mp)
+        except Exception as ex:  # pylint: disable=W0702
+            msg = _("Faild to detach volume %(volume_id)s from %(mp)s")
+            LOG.warning(msg % locals(), context=context, instance=instance)
+            raise ex
+        else:
+            # FIXME(wangpan): we sleep here to avoid attaching/detaching
+            #                 too frequently(vm may get exception if do that).
+            time.sleep(FLAGS.attch_detach_interval)
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def detach_nbs_volume(self, context, volume_id, instance):
+        """Detach a nbs volume from an instance."""
+        bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                            volume_id, True)
+        if bdm is None:
+            LOG.warn(_("nbs volume %s is not attached") % volume_id,
+                        context=context, instance=instance)
+        else:
+            try:
+                self._detach_nbs_volume(context, instance, bdm)
+            except Exception:
+                # FIXME(wangpan): if libvirt raise an exception or any other
+                #                 exception was raised while detaching, we
+                #                 ignore it and always call nbs to detach it.
+                pass
+            else:
+                self.db.block_device_mapping_destroy_by_instance_and_volume(
+                        context, instance['uuid'], volume_id)
+
+        # call nbs to detach volume from host
+        host_ip = self._get_host_ip_by_ifname(FLAGS.host_ip_ifname)
+        # we don't check the result of nbs detachment
+        self.nbs_api.detach(context, volume_id, host_ip)
+
     def _detach_volume(self, context, instance, bdm):
         """Do the actual driver detach using block device mapping."""
         mp = bdm['device_name']
@@ -3154,6 +3455,10 @@ class ComputeManager(manager.SchedulerDependentManager):
                  instance=instance_ref)
 
         # Detaching volumes.
+        # FIXME(wangpan): we need to prep nbs volumes automatically during
+        #                 all migrations such as resize, migrate, live
+        #                 migration, relocate...
+        #                 but currently we don't do this...
         for bdm in self._get_instance_volume_bdms(ctxt, instance_ref['uuid']):
             # NOTE(vish): We don't want to actually mark the volume
             #             detached, or delete the bdm, just remove the
@@ -3282,12 +3587,16 @@ class ComputeManager(manager.SchedulerDependentManager):
         self.network_api.setup_networks_on_host(context, instance_ref,
                                                          self.host)
 
+        # FIXME(wangpan): we need to prep nbs volumes automatically during
+        #                 all migrations such as resize, migrate, live
+        #                 migration, relocate...
+        #                 but currently we don't do this...
         for bdm in self._get_instance_volume_bdms(context,
                                                   instance_ref['uuid']):
             volume_id = bdm['volume_id']
             volume = self.volume_api.get(context, volume_id)
-            self.compute_rpcapi.remove_volume_connection(context, instance_ref,
-                    volume['id'], dest)
+            self.compute_rpcapi.remove_volume_connection(context,
+                    instance_ref, volume['id'], dest)
 
         # Block migration needs empty image at destination host
         # before migration starts, so if any failure occurs,
diff --git a/nova/compute/nbs_client.py b/nova/compute/nbs_client.py
new file mode 100644
index 0000000..e23594f
--- /dev/null
+++ b/nova/compute/nbs_client.py
@@ -0,0 +1,352 @@
+# author: hzwangpan@corp.netease.com
+# client of nbs api for nova to interact with nbs service
+
+import httplib
+import json
+import time
+
+from nova import flags
+from nova.openstack.common import cfg
+from nova.openstack.common import log as logging
+from nova.openstack.common.notifier import api as notifier_api
+
+
+FLAGS = flags.FLAGS
+LOG = logging.getLogger(__name__)
+
+
+class API():
+    def __init__(self):
+        self._nbs_api_server = FLAGS.nbs_api_server or ""
+        if "http://" in self._nbs_api_server:
+            self._nbs_api_server = self._nbs_api_server.replace("http://", "")
+        if "/" in self._nbs_api_server:
+            self._nbs_api_server = self._nbs_api_server.replace("/", "")
+
+    def get(self, context, volume_id):
+        """Get volume detail by calling nbs DescribeVolumes api"""
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=DescribeVolumes"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id))
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def attach(self, context, volume_id, instance_uuid, host_ip, device):
+        """Attach a nbs volume to host by calling nbs AttachVolume api"""
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not instance_uuid:
+            LOG.warn(_("instance uuid should be given, do nothing "
+                        "and return now"), context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not device:
+            LOG.warn(_("device should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=AttachVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&InstanceId=" + str(instance_uuid)
+                + "&HostIp=" + str(host_ip)
+                + "&Device=" + str(device))
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def detach(self, context, volume_id, host_ip):
+        """Detach a nbs volume from host by calling nbs DetachVolume api"""
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=DetachVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&HostIp=" + host_ip)
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def extend(self, context, volume_id, size, host_ip=None):
+        """Extend a nbs volume by calling nbs ExtendVolume api"""
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not size:
+            LOG.warn(_("size is should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=ExtendVolume"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&Size=" + str(size))
+        if host_ip:
+            url += "&HostIp=" + host_ip
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def wait_for_attached(self, context, volume_id, instance_uuid,
+                            check_interval=3):
+        """
+        Wait for nbs finished to attach the volume to host by checking status
+        """
+        # FIXME(wangpan): How to deal with the timeout situation?
+        start = time.time()
+        times = 1
+        while (time.time() - start < FLAGS.nbs_attach_wait_timeout):
+            # FIXME(wangpan): we just deal with single attachment status now
+            try:
+                volume = self.get(context, volume_id)["volumes"][0]
+            except (IndexError, KeyError, TypeError):
+                LOG.warn(_("Get nothing from nbs server, sleep %ds and retry, "
+                    "times: %d") % (check_interval, times), context=context)
+                time.sleep(check_interval)
+                times += 1
+                continue
+
+            try:
+                attachment = volume["attachments"][0]
+                if attachment["status"] == "attached":
+                    return True
+            except (IndexError, KeyError, TypeError):
+                LOG.warn(_("Get wrong info from nbs server, sleep %ds and "
+                        "retry, times: %d") % (check_interval, times),
+                        context=context)
+
+            LOG.info(_("sleep %ds and retry to check volume's status, "
+                    "times: %d") % (check_interval, times), context=context)
+            time.sleep(check_interval)
+            times += 1
+        LOG.warn(_("volume %s can not be attached successfully after %ds") %
+                 (volume_id, FLAGS.nbs_attach_wait_timeout), context=context)
+        return False
+
+    def wait_for_extended(self, context, volume_id, expected_size,
+                            check_interval=3):
+        """Wait for nbs finished to extend the volume by checking status"""
+        # FIXME(wangpan): How to deal with the timeout situation?
+        start = time.time()
+        times = 1
+        while (time.time() - start < FLAGS.nbs_extend_wait_timeout):
+            # FIXME(wangpan): we just deal with single attachment status now
+            try:
+                volume = self.get(context, volume_id)["volumes"][0]
+            except (IndexError, KeyError, TypeError):
+                LOG.warn(_("Get nothing from nbs server, sleep %ds and retry, "
+                    "times: %d") % (check_interval, times), context=context)
+                time.sleep(check_interval)
+                times += 1
+                continue
+
+            try:
+                if int(volume["size"]) == expected_size:
+                    return True
+            except (KeyError, TypeError, ValueError):
+                LOG.warn(_("Get wrong info from nbs server, sleep %ds and "
+                        "retry, times: %d") % (check_interval, times),
+                        context=context)
+
+            LOG.info(_("sleep %ds and retry to check volume's size, "
+                    "times: %d") % (check_interval, times), context=context)
+            time.sleep(check_interval)
+            times += 1
+        LOG.warn(_("volume %s cannot be extended successfully after %ds") %
+                 (volume_id, FLAGS.nbs_extend_wait_timeout), context=context)
+        return False
+
+    def get_host_dev_and_qos_info(self, context, volume_id, host_ip):
+        """Get host device and QoS info from nbs server"""
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=GetVolumeQos"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&HostIp=" + str(host_ip))
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def notify_nbs_libvirt_result(self, context, volume_id, operation, result,
+                                device, host_ip, instance_uuid):
+        """
+        Tell nbs the operation result of libvirt, so they can get a identical
+        status with us.
+        """
+        if not volume_id:
+            LOG.warn(_("volume id should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not operation:
+            LOG.warn(_("operation should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if result not in (True, False):
+            LOG.warn(_("result should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not device:
+            LOG.warn(_("device should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not host_ip:
+            LOG.warn(_("host ip should be given, do nothing and return now"),
+                        context=context)
+            return None
+
+        if not instance_uuid:
+            LOG.warn(_("instance uuid should be given, do nothing "
+                        "and return now"), context=context)
+            return None
+
+        project_id = context.project_id
+        url = ("/" + FLAGS.nbs_prefix_url + "/?Action=NotifyState"
+                + "&ProjectId=" + str(project_id)
+                + "&VolumeId=" + str(volume_id)
+                + "&OperateType=" + str(operation)
+                + "&Device=" + str(device)
+                + "&HostIp=" + str(host_ip)
+                + "&InstanceId=" + str(instance_uuid))
+        if result:
+            url += "&OperateState=" + "success"
+        else:
+            url += "&OperateState=" + "fail"
+
+        headers = {"Content-type": "application/json",
+                   "Accept": "application/json"}
+        params = None
+        method = "GET"
+        return self._request(context, method, url, params, headers)
+
+    def _request(self, context, method, url, params, headers):
+        """The essential implement of nbs api client"""
+        if not self._nbs_api_server:
+            LOG.warn(_("nbs_api_server is null, can't connect to it"))
+            return None
+
+        unified_log_id = None
+        unified_log_seq = None
+        if context:
+            unified_log_id = context.to_dict().get('unified_log_id', None)
+            unified_log_seq = context.to_dict().get('unified_log_seq', None)
+        if unified_log_id and unified_log_seq:
+            log_seq_nums = unified_log_seq.split('.')
+            log_seq_nums[-1] = str(int(log_seq_nums[-1]) + 1)
+            new_log_seq = '.'.join(log_seq_nums)
+            context.unified_log_seq = new_log_seq
+
+            url = url + "&LogId=" + str(unified_log_id)
+            url = url + "&LogSeq=" + str(new_log_seq)
+
+        full_url = self._nbs_api_server + url
+
+        LOG.info(_("send request to %(full_url)s, method: %(method)s, "
+                   "body: %(params)s, headers: %(headers)s") % locals(),
+                   context=context)
+        try:
+            nbs_conn = httplib.HTTPConnection(self._nbs_api_server)
+        except Exception, ex:
+            LOG.error(_("exception occurs when connect to nbs server, "
+                        "error msg: %s") % str(ex), context=context)
+            self._notify_NBS_connection_failure(context, self._nbs_api_server)
+            return None
+
+        try:
+            nbs_conn.request(method, url, params, headers)
+            resp = nbs_conn.getresponse()
+            if resp:
+                if resp.status == 200:
+                    data = json.loads(resp.read())
+                    request_id = data["requestId"]
+                    LOG.info(_("request id: %(request_id)s, data: "
+                               "%(data)s") % locals(), context=context)
+                    return data
+                else:
+                    err_code = resp.status
+                    err_reason = resp.reason
+                    LOG.error(_("error occurs when contact to nbs server, "
+                                "error code: %(err_code)d, "
+                                "reason: %(err_reason)s") % locals(),
+                                context=context)
+                    self._notify_NBS_connection_failure(context, full_url)
+                    return None
+            else:
+                LOG.error(_("nbs server doesn't return any response"),
+                            context=context)
+                self._notify_NBS_connection_failure(context, full_url)
+                return None
+
+        except Exception, ex:
+            LOG.error(_("exception occurs when send request to nbs server, "
+                        "error msg: %s") % str(ex), context=context)
+            self._notify_NBS_connection_failure(context, full_url)
+            return None
+        finally:
+            nbs_conn.close()
+
+    def _notify_NBS_connection_failure(self, context, url):
+        """Send a message to notification about NBS connection failure"""
+        try:
+            LOG.info(_('notify NBS connection failure'))
+            payload = dict({'url': url})
+            notifier_api.notify(context,
+                                notifier_api.publisher_id('api_nbs'),
+                                'api_nbs.nvs_connect_nbs_failure',
+                                notifier_api.ERROR, payload)
+        except Exception:
+            LOG.exception(_('notification module error when do notifying '
+                            'NVS connect NBS failed.'))
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index 0cd2668..a3f9fc9 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -170,6 +170,19 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 instance=instance_p, network_id=network_id),
                 topic=_compute_topic(self.topic, ctxt, None, instance))
 
+    def extend_nbs_volume(self, ctxt, instance, volume_id, size, device=None):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('extend_nbs_volume',
+                instance=instance_p, volume_id=volume_id,
+                size=size, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
+    def attach_nbs_volume(self, ctxt, instance, volume_id, device):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('attach_nbs_volume',
+                instance=instance_p, volume_id=volume_id, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def attach_volume(self, ctxt, instance, volume_id, mountpoint):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('attach_volume',
@@ -210,6 +223,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 reservations=reservations),
                 topic=_compute_topic(self.topic, ctxt, host, instance))
 
+    def detach_nbs_volume(self, ctxt, instance, volume_id):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('detach_nbs_volume',
+                instance=instance_p, volume_id=volume_id),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def detach_volume(self, ctxt, instance, volume_id):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('detach_volume',
@@ -458,6 +477,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
         topic = _compute_topic(self.topic, ctxt, host, None)
         return self.call(ctxt, self.make_msg('get_host_uptime'), topic)
 
+    def get_device_for_nbs_volume(self, ctxt, instance):
+        instance_p = jsonutils.to_primitive(instance)
+        return self.call(ctxt, self.make_msg('get_device_for_nbs_volume',
+                instance=instance_p),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def reserve_block_device_name(self, ctxt, instance, device):
         instance_p = jsonutils.to_primitive(instance)
         return self.call(ctxt, self.make_msg('reserve_block_device_name',
@@ -492,10 +517,10 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 instance=instance_p),
                 topic=_compute_topic(self.topic, ctxt, None, instance))
 
-    def terminate_instance(self, ctxt, instance, del_ebs=False):
+    def terminate_instance(self, ctxt, instance):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('terminate_instance',
-                instance=instance_p, del_ebs=del_ebs),
+                instance=instance_p),
                 topic=_compute_topic(self.topic, ctxt, None, instance))
 
     def unpause_instance(self, ctxt, instance):
diff --git a/nova/compute/utils.py b/nova/compute/utils.py
index a8a0a65..f331510 100644
--- a/nova/compute/utils.py
+++ b/nova/compute/utils.py
@@ -55,7 +55,6 @@ def add_instance_fault_from_exc(context, instance_uuid, fault, exc_info=None):
     }
     db.instance_fault_create(context, values)
 
-
 def get_device_name_for_instance(context, instance, device):
     """Validates (or generates) a device name for instance.
 
diff --git a/nova/db/api.py b/nova/db/api.py
index 68b3cf2..118881f 100644
--- a/nova/db/api.py
+++ b/nova/db/api.py
@@ -1321,7 +1321,11 @@ def block_device_mapping_update_or_create(context, values):
 def block_device_mapping_get_all_by_instance(context, instance_uuid):
     """Get all block device mapping belonging to an instance"""
     return IMPL.block_device_mapping_get_all_by_instance(context,
-                                                         instance_uuid)
+        instance_uuid)
+
+def block_device_mapping_get_all_by_volume(context, volume_id):
+    """Get all block device mapping by volume"""
+    return IMPL.block_device_mapping_get_all_by_instance(context, volume_id)
 
 
 def block_device_mapping_destroy(context, bdm_id):
diff --git a/nova/db/sqlalchemy/api.py b/nova/db/sqlalchemy/api.py
index fef3f99..297e871 100644
--- a/nova/db/sqlalchemy/api.py
+++ b/nova/db/sqlalchemy/api.py
@@ -3741,6 +3741,12 @@ def block_device_mapping_get_all_by_instance(context, instance_uuid):
                  filter_by(instance_uuid=instance_uuid).\
                  all()
 
+@require_context
+def block_device_mapping_get_all_by_volume(context, volume_id):
+    return _block_device_mapping_get_query(context).\
+                 filter_by(volume_id=volume_id).\
+                 all()
+
 
 @require_context
 def block_device_mapping_destroy(context, bdm_id):
diff --git a/nova/exception.py b/nova/exception.py
index 2c3e7bb..3fdee4e 100644
--- a/nova/exception.py
+++ b/nova/exception.py
@@ -378,6 +378,10 @@ class DevicePathInUse(Invalid):
     message = _("The supplied device path (%(path)s) is in use.")
 
 
+class NoFreeDevice(Invalid):
+    message = _("Cannot find a free device to attach nbs volume.")
+
+
 class DeviceIsBusy(Invalid):
     message = _("The supplied device (%(device)s) is busy.")
 
@@ -1141,6 +1145,22 @@ class EcuKeyError(NovaException):
     message = _("Key 'ecus_per_vcpu:' may not be set")
 
 
+class NbsException(NovaException):
+    message = _("Nbs error occurred")
+
+
+class NbsTimeout(NovaException):
+    message = _("Nbs operation timeout")
+
+
+class NotSupported(NovaException):
+    message = _("Operation is not supported currently")
+
+
+class MemCacheClientNotFound(NotFound):
+    message = _("Memory cache client is not found")
+
+
 class TcInvalid(Invalid):
     message = _("%(err)s")
 
diff --git a/nova/flags.py b/nova/flags.py
index c0112ce..e889f23 100644
--- a/nova/flags.py
+++ b/nova/flags.py
@@ -230,6 +230,41 @@ resize_opts = [
 
 FLAGS.register_opts(resize_opts)
 
+nbs_opts = [
+    cfg.StrOpt('ebs_backend',
+                default='nbs',
+                help='the backend type of ebs service, may be nbs or cinder'),
+    cfg.StrOpt('nbs_api_server',
+                default=None,
+                help='the host and port of nbs server'),
+    cfg.StrOpt('nbs_prefix_url',
+                default='EBS',
+                help='the url prefix of nbs server, the final url should like '
+                 'this: nbs_api_server/nbs_prefix_url/?Action=XX&YY=ZZ...'),
+    cfg.StrOpt('host_ip_ifname',
+               default='br100',
+               help='network device to get intranet ip address for nbs'),
+    cfg.IntOpt('nbs_boot_wait_timeout',
+               default=120,
+               help='nbs volume wait timeout during init host(in second)'),
+    cfg.IntOpt('nbs_attach_wait_timeout',
+               default=30,
+               help='nbs volume wait timeout during attachment(in second)'),
+    cfg.IntOpt('nbs_extend_wait_timeout',
+               default=30,
+               help='nbs volume wait timeout during extension(in second)'),
+    cfg.IntOpt('attch_detach_interval',
+               default=5,
+               help='interval of nbs volume between attachment and '
+                    'detachment(in second)'),
+    cfg.StrOpt('nbs_mountpoint_prefix',
+               default='/dev/nbs/xd',
+               help='mount point prefix of nbs disk displays in vm, may like '
+                    '/dev/nbs/xdaz'),
+]
+
+FLAGS.register_opts(nbs_opts)
+
 global_opts = [
     cfg.StrOpt('my_ip',
                default=_get_my_ip(),
diff --git a/nova/service.py b/nova/service.py
index a9382e2..d81fd71 100644
--- a/nova/service.py
+++ b/nova/service.py
@@ -20,6 +20,7 @@
 """Generic Node base class for all workers that run on hosts."""
 
 import errno
+import fcntl
 import inspect
 import os
 import random
@@ -99,9 +100,6 @@ service_opts = [
     cfg.IntOpt('osapi_volume_workers',
                default=None,
                help='Number of workers for OpenStack Volume API service'),
-    cfg.StrOpt('host_ip_ifname',
-               default='',
-               help='Network device to get intranet ip address'),
     ]
 
 FLAGS = flags.FLAGS
@@ -509,7 +507,6 @@ class Service(object):
     def _get_host_ip_by_ifname(self, ifname):
         s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
         try:
-            import fcntl
             ip = socket.inet_ntoa(fcntl.ioctl(
                                               s.fileno(),
                                               0x8915,  # SIOCGIFADDR
diff --git a/nova/virt/driver.py b/nova/virt/driver.py
index 8be6ff7..3064ab7 100644
--- a/nova/virt/driver.py
+++ b/nova/virt/driver.py
@@ -233,10 +233,27 @@ class ComputeDriver(object):
         # TODO(Vek): Need to pass context in for access to auth_token
         raise NotImplementedError()
 
+    def get_device_for_nbs_volume(self, instance_name, used_dev):
+        """Generates a device name for attaching nbs volume"""
+        raise NotImplementedError()
+
+    def extend_nbs_volume(self, instance_name, device, size):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
     def attach_volume(self, connection_info, instance_name, mountpoint):
         """Attach the disk to the instance at mountpoint using info"""
         raise NotImplementedError()
 
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        """Detach the disk attached to the instance"""
+        raise NotImplementedError()
+
     def detach_volume(self, connection_info, instance_name, mountpoint):
         """Detach the disk attached to the instance"""
         raise NotImplementedError()
diff --git a/nova/virt/libvirt/config.py b/nova/virt/libvirt/config.py
index 5b63919..3c33701 100644
--- a/nova/virt/libvirt/config.py
+++ b/nova/virt/libvirt/config.py
@@ -364,6 +364,8 @@ class LibvirtConfigGuestDisk(LibvirtConfigGuestDevice):
         self.auth_secret_type = None
         self.auth_secret_uuid = None
         self.serial = None
+        self.iotune = None
+        self.slot = None
 
     def format_dom(self):
         dev = super(LibvirtConfigGuestDisk, self).format_dom()
@@ -408,6 +410,28 @@ class LibvirtConfigGuestDisk(LibvirtConfigGuestDevice):
         if self.serial is not None:
             dev.append(self._text_node("serial", self.serial))
 
+        if self.slot is not None:
+            address = etree.Element("address")
+            address.set("type", "pci")
+            address.set("domain", "0x0000")
+            address.set("bus", "0x00")
+            address.set("slot", "0x%0.2x" % self.slot)
+            address.set("function", "0x0")
+            dev.append(address)
+
+        if self.iotune is not None:
+            total_bytes_sec = self.iotune.get("iotune_total_bytes", None)
+            total_iops_sec = self.iotune.get("iotune_total_iops", None)
+            if total_bytes_sec is not None or total_iops_sec is not None:
+                iotune = etree.Element("iotune")
+                if total_bytes_sec is not None:
+                    iotune.append(self._text_node("total_bytes_sec",
+                                                    total_bytes_sec))
+                if total_iops_sec is not None:
+                    iotune.append(self._text_node("total_iops_sec",
+                                                    total_iops_sec))
+                dev.append(iotune)
+
         return dev
 
 
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 5bd7529..a57ca9a 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -734,6 +734,148 @@ class LibvirtDriver(driver.ComputeDriver):
         method = getattr(driver, method_name)
         return method(connection_info, *args, **kwargs)
 
+    def _slot_to_disk(self, slot):
+        ascii_table = "abcdefghijklmnopqrstuvwxyz"
+        nums = slot * 8
+        nbs_disk = ''
+        while nums > 0:
+            value = nums % 26
+            ch = ascii_table[value:value + 1]
+            nbs_disk = ch + nbs_disk
+            nums = nums / 26
+        return nbs_disk
+
+    # FIXME(wangpan): we find that only slot id is non-zero in pci address,
+    #                 such as: <address type='pci'
+    #                           domain='0x0000'
+    #                           bus='0x00'
+    #                           slot='0x03'
+    #                           function='0x0'/>
+    #                 so we only find a free slot of domain.
+    def _find_free_dev_and_pcislot(self, instance_name, used_dev):
+        virt_dom = self._lookup_by_name(instance_name)
+        dom_xml = virt_dom.XMLDesc(0)
+        doc = etree.fromstring(dom_xml)
+        devices = doc.findall('./devices/')
+        used_pci_slot_ids = [int(device.find('address').attrib['slot'], 16)
+                                for device in devices
+                                    if device.find('address') is not None]
+        disks = doc.findall('./devices/disk')
+        used_disk_devs = [disk.find('target').attrib['dev'] for disk in disks]
+
+        if used_dev is not None:
+            if used_dev.get('slot', None) is not None:
+                used_pci_slot_ids.append(used_dev['slot'])
+            if used_dev.get('dev', None) is not None:
+                used_disk_devs.append(used_dev['dev'])
+
+        free_slot = None
+        free_dev = None
+
+        # slot id range is 0-31
+        for id in range(31, 0, -1):
+            if id not in used_pci_slot_ids:
+                free_slot = id
+                break
+
+        # vd[d-z], skip root, ephemeral and swap disk
+        for i in range(100, 123):
+            dev = 'vd' + chr(i)
+            if dev not in used_disk_devs:
+                free_dev = dev
+                break
+
+        return (free_slot, free_dev)
+
+
+    def get_device_for_nbs_volume(self, instance_name, used_dev):
+        """Generates a device name for attaching nbs volume"""
+        free_slot, free_dev = self._find_free_dev_and_pcislot(instance_name,
+                                                                used_dev)
+        if free_slot is None or free_dev is None:
+            return None
+
+        real_path = FLAGS.nbs_mountpoint_prefix + self._slot_to_disk(free_slot)
+        target_dev = '/dev/' + free_dev
+
+        return {'mountpoint': target_dev,
+                'real_path': real_path,
+                'slot': free_slot}
+
+    @exception.wrap_exception()
+    def extend_nbs_volume(self, instance_name, device, size):
+        """Tell instance the nbs volume has been resized"""
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        mount_device = device['mountpoint'].rpartition("/")[2]
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                size_KB = size * 1024 * 1024
+                virt_dom.blockResize(mount_device, size_KB, 0)
+        except Exception as ex:
+            LOG.exception(ex)
+            raise ex
+
+    @exception.wrap_exception()
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """
+        Attach a nbs volume to instance, and check the device or slot is
+        in-use, return retry if in-use, if need retry, the used device is
+        returned, too.
+        """
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        conf = config.LibvirtConfigGuestDisk()
+        conf.source_type = "block"
+        conf.driver_name = libvirt_utils.pick_disk_driver_name(
+                                            is_block_dev=True)
+        conf.driver_format = "raw"
+        conf.driver_cache = "none"
+        conf.source_path = host_dev
+        conf.target_dev = device['mountpoint'].rpartition("/")[2]
+        conf.target_bus = "virtio"
+        conf.serial = volume_id
+        conf.slot = device['slot']
+        conf.iotune = qos_info
+
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            # NOTE(vish): We can always affect config because our
+            #             domains are persistent, but we should only
+            #             affect live if the domain is running.
+            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+            virt_dom.attachDeviceFlags(conf.to_xml(), flags)
+            return (False, None)
+        except Exception as ex:
+            if isinstance(ex, libvirt.libvirtError):
+                errcode = ex.get_error_code()
+                if errcode == libvirt.VIR_ERR_OPERATION_FAILED:
+                    raise exception.DeviceIsBusy(device=disk_dev)
+                elif errcode == libvirt.VIR_ERR_INVALID_ARG:
+                    # target dev already exists
+                    return (True, {'dev': mount_device})
+                elif errcode == libvirt.VIR_ERR_XML_ERROR:
+                    # pci slot already used
+                    return (True, {'slot': slot})
+                else:
+                    errmsg = ex.get_error_message()
+                    LOG.error(_("Error code %d, message %s, instance "
+                                "name %s, volume_id %s") % (errcode,
+                                    errmsg, instance_name, volume_id))
+                    raise ex
+            else:
+                raise ex
+
     @exception.wrap_exception()
     def attach_volume(self, connection_info, instance_name, mountpoint):
         virt_dom = self._lookup_by_name(instance_name)
@@ -792,6 +934,39 @@ class LibvirtDriver(driver.ComputeDriver):
         return xml
 
     @exception.wrap_exception()
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        """Detach a nbs volume from an instance"""
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise exception.NotSupported()
+
+        mount_device = mountpoint.rpartition("/")[2]
+        try:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still logout even if
+            #             the instance doesn't exist here anymore.
+            virt_dom = self._lookup_by_name(instance_name)
+            xml = self._get_disk_xml(virt_dom.XMLDesc(0), mount_device)
+            if not xml:
+                raise exception.DiskNotFound(location=mount_device)
+            else:
+                flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+                state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+                if state == power_state.RUNNING:
+                    flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+                virt_dom.detachDeviceFlags(xml, flags)
+        except libvirt.libvirtError as ex:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still disconnect even if
+            #             the instance doesn't exist here anymore.
+            error_code = ex.get_error_code()
+            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
+                # NOTE(vish):
+                LOG.warn(_("During detach_volume, instance disappeared."))
+            else:
+                raise ex
+
+    @exception.wrap_exception()
     def detach_volume(self, connection_info, instance_name, mountpoint):
         mount_device = mountpoint.rpartition("/")[2]
         try:
@@ -2091,13 +2266,29 @@ class LibvirtDriver(driver.ComputeDriver):
                         nova_context.get_admin_context(), instance['uuid'],
                         {'default_swap_device': '/dev/' + swap_device})
 
-                for vol in block_device_mapping:
-                    connection_info = vol['connection_info']
-                    mount_device = vol['mount_device'].rpartition("/")[2]
-                    cfg = self.volume_driver_method('connect_volume',
-                                                    connection_info,
-                                                    mount_device)
-                    devices.append(cfg)
+                if FLAGS.ebs_backend == 'nbs':
+                    for vol in block_device_mapping:
+                        cfg = config.LibvirtConfigGuestDisk()
+                        cfg.source_type = "block"
+                        cfg.driver_name = libvirt_utils.pick_disk_driver_name(
+                                                            is_block_dev=True)
+                        cfg.driver_format = "raw"
+                        cfg.driver_cache = "none"
+                        cfg.source_path = vol['host_dev']
+                        cfg.target_dev = vol['mount_device'].rpartition("/")[2]
+                        cfg.target_bus = "virtio"
+                        cfg.serial = vol['serial']
+                        cfg.slot = vol['slot']
+                        cfg.iotune = vol['qos_info']
+                        devices.append(cfg)
+                else:
+                    for vol in block_device_mapping:
+                        connection_info = vol['connection_info']
+                        mount_device = vol['mount_device'].rpartition("/")[2]
+                        cfg = self.volume_driver_method('connect_volume',
+                                                        connection_info,
+                                                        mount_device)
+                        devices.append(cfg)
 
             if (instance.get('config_drive') or
                 instance.get('config_drive_id') or
@@ -2405,15 +2596,19 @@ class LibvirtDriver(driver.ComputeDriver):
                     break
             return (remove_dev_notfound, etree.tostring(domain_root))
 
-        block_device_mapping = driver.block_device_info_get_mapping(
-            block_device_info)
-
-        for vol in block_device_mapping:
-            connection_info = vol['connection_info']
-            mount_device = vol['mount_device'].rpartition("/")[2]
-            self.volume_driver_method('connect_volume',
-                                      connection_info,
-                                      mount_device)
+        # FIXME(wangpan): I wish we have already attached nbs volume to host
+        if FLAGS.ebs_backend == 'nbs':
+            pass
+        else:
+            block_device_mapping = driver.block_device_info_get_mapping(
+                block_device_info)
+
+            for vol in block_device_mapping:
+                connection_info = vol['connection_info']
+                mount_device = vol['mount_device'].rpartition("/")[2]
+                self.volume_driver_method('connect_volume',
+                                          connection_info,
+                                          mount_device)
 
         self.plug_vifs(instance, network_info)
         self.firewall_driver.setup_basic_filtering(instance, network_info)
