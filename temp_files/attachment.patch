commit 63b833ccf24ed0bb0b85341ddfcec6983f19e209
Author: Wangpan <hzwangpan@corp.netease.com>
Date:   Thu May 16 17:59:56 2013 +0800

    decouple nbs and nvs
    
    1. add nbs volume attachment process
    2. add nbs volume detachment process
    3. add nbs volume extension process
    
    Change-Id: Ia9f003f498a3748028ce89879d6eaf5685bcad94

diff --git a/nova/api/openstack/compute/contrib/server_status.py b/nova/api/openstack/compute/contrib/server_status.py
index d9368a4..175230e 100644
--- a/nova/api/openstack/compute/contrib/server_status.py
+++ b/nova/api/openstack/compute/contrib/server_status.py
@@ -14,19 +14,14 @@
 #    License for the specific language governing permissions and limitations
 #    under the License
 
-import datetime
-
-import memcache
 from webob import exc
 
 from nova.api.openstack import extensions
-from nova import db
+from nova import compute
 from nova import exception
 from nova import flags
 from nova.openstack.common import cfg
 from nova.openstack.common import log as logging
-from nova.openstack.common import timeutils
-from nova import utils
 
 
 metadata_opts = [
@@ -39,64 +34,27 @@ FLAGS = flags.FLAGS
 FLAGS.register_opts(metadata_opts)
 LOG = logging.getLogger(__name__)
 authorize = extensions.extension_authorizer('compute', 'server_status')
-MEMCACHE_CLIENT_API = None
 
 
 class ServerStatusController(object):
+    def __init__(self):
+        self.compute_api = compute.API()
 
     def show(self, req, id):
+        context = req.environ['nova.context']
+        authorize(context)
+        LOG.debug(_("Listing server status"), context=context,
+                    instance_uuid=id)
+
         try:
-            context = req.environ['nova.context']
-            authorize(context)
-            LOG.debug(_("Listing server status"))
-            result = self._instance_os_boot_ready(
-                            context, id,
-                            FLAGS.server_heartbeat_period)
+            instance = self.compute_api.get(context, id)
+            result = self.compute_api.instance_os_boot_ready(context,
+                                            instance['uuid'],
+                                            FLAGS.server_heartbeat_period)
             return result
         except exception.NotFound:
             raise exc.HTTPNotFound()
 
-    def _instance_os_boot_ready(self, context, instance_uuid,
-                                heartbeat_period):
-        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
-        status = 'down'
-        interval = heartbeat_period * 1.5
-        try:
-            db.instance_get_by_uuid(context, instance_uuid)
-        except exception.InstanceNotFound:
-            raise exception.NotFound()
-
-        cache_key = str(instance_uuid + '_heart')
-        memcache_client = self._get_memcache_client()
-        if memcache_client is not None:
-            cache_value = memcache_client.get(cache_key)
-            if cache_value:
-                last_heartbeat = datetime.datetime.strptime(
-                                        cache_value,
-                                        '%Y-%m-%d %H:%M:%S')
-                # Timestamps in DB are UTC.
-                elapsed = utils.total_seconds(
-                                timeutils.utcnow() - last_heartbeat)
-                if abs(elapsed) <= interval:
-                    status = 'up'
-        else:
-            msg = _('Store data can not catch')
-            raise exc.HTTPServerError(explanation=msg)
-
-        return {'instance_uuid': instance_uuid,
-                'status': status}
-
-    def _get_memcache_client(self):
-        """Return memcache client"""
-        global MEMCACHE_CLIENT_API
-        if MEMCACHE_CLIENT_API is not None:
-            return MEMCACHE_CLIENT_API
-        else:
-            if FLAGS.memcached_servers:
-                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
-                                                      debug=0)
-            return MEMCACHE_CLIENT_API
-
 
 class Server_status(extensions.ExtensionDescriptor):
     """Add server_status to the Create Server v1.1 API"""
diff --git a/nova/api/openstack/compute/contrib/volumes.py b/nova/api/openstack/compute/contrib/volumes.py
index 9940e30..f71a390 100644
--- a/nova/api/openstack/compute/contrib/volumes.py
+++ b/nova/api/openstack/compute/contrib/volumes.py
@@ -15,6 +15,7 @@
 
 """The volumes extension."""
 
+import time
 import webob
 from webob import exc
 from xml.dom import minidom
@@ -164,6 +165,7 @@ class VolumeController(wsgi.Controller):
     """The Volumes API controller for the OpenStack API."""
 
     def __init__(self):
+        self.compute_api = compute.API()
         self.volume_api = volume.API()
         super(VolumeController, self).__init__()
 
@@ -271,6 +273,53 @@ class VolumeController(wsgi.Controller):
 
         return wsgi.ResponseObject(result, headers=dict(location=location))
 
+    def _extend_nbs_volume(self, req, id, body):
+        """ """
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if not self.is_valid_body(body, 'size'):
+            raise exc.HTTPUnprocessableEntity()
+        try:
+            instance_uuid = self.compute_api.check_nbs_attached(context, id)
+        except NotFound:
+            raise NotFound
+        size = int(body['size']) # GB
+        # call nbs to extend this volume firstly
+
+        if instance_uuid:
+            try:
+                # check instance exists
+                instance = self.compute_api.get(context, instance_uuid)
+                # Check os status if instance is 'ACTIVE'
+                if instance['status'] == "ACTIVE":
+                    server_heartbeat_period = FLAGS.get(
+                                                'server_heartbeat_period', 10)
+                    os_status = self.compute_api.instance_os_boot_ready(
+                                                    context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                    if os_status['status'] != "up":
+                        explanation = _("Can't extend volume while instance "
+                                        "os is starting.")
+                        raise exc.HTTPForbidden(explanation=explanation)
+            except NotFound:
+                raise NotFound
+
+            # notify instance about this extension
+            self.compute_api.extend_nbs_volume(context, instance, id, size)
+        else:
+            return {}
+
+    # @wsgi.serializers(xml=VolumesTemplate)
+    def update(self, req, id, body):
+        """Extend a exists nbs volume."""
+        if FLAGS.ebs_backend == 'nbs':
+            return self._extend_nbs_volume(req, id, body)
+        else:
+            raise exc.HTTPBadRequest()
+
+
 
 def _translate_attachment_detail_view(volume_id, instance_uuid, mountpoint):
     """Maps keys for attachment details view."""
@@ -375,9 +424,66 @@ class VolumeAttachmentController(wsgi.Controller):
             instance['uuid'],
             assigned_mountpoint)}
 
+    def _attach_nbs_volume(self, req, server_id, body):
+        """   """
+        context = req.environ['nova.context']
+        authorize(context)
+
+        if not self.is_valid_body(body, 'volumeAttachment'):
+            raise exc.HTTPUnprocessableEntity()
+
+        volume_id = body['volumeAttachment']['volumeId']
+
+        msg = _("Attach nbs volume %s to instance %s") % (volume_id, server_id)
+        LOG.audit(msg, context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+            # TODO(wangpan): if the instance forbid to attach nbs volume,
+            #                raise exception.
+            # Check os status if instance is 'ACTIVE'
+            if instance['status'] == "ACTIVE":
+                server_heartbeat_period = FLAGS.get('server_heartbeat_period',
+                                                    10)
+                os_status = self.compute_api.instance_os_boot_ready(context,
+                                                    instance['uuid'],
+                                                    server_heartbeat_period)
+                if os_status['status'] != "up":
+                    explanation = _("Can't attach volume while instance os "
+                                    "is starting.")
+                    raise exc.HTTPForbidden(explanation=explanation)
+            device = self.compute_api.attach_nbs_volume(context, instance,
+                                                        volume_id)
+        except exception.NotFound:
+            raise exc.HTTPNotFound()
+
+        # The attach is async
+        attachment = {}
+        attachment['requestId'] = context.request_id
+        attachment['instanceId'] = instance['uuid']
+        attachment['volumeId'] = volume_id
+        attachment['device'] = device
+        attachment['status'] = 0
+        attachment['attachTime'] = long(time.time())
+
+        # NOTE(justinsb): And now, we have a problem...
+        # The attach is async, so there's a window in which we don't see
+        # the attachment (until the attachment completes).  We could also
+        # get problems with concurrent requests.  I think we need an
+        # attachment state, and to write to the DB here, but that's a bigger
+        # change.
+        # For now, we'll probably have to rely on libraries being smart
+
+        # TODO(justinsb): How do I return "accepted" here?
+        return {'attachment': attachment}
+
     @wsgi.serializers(xml=VolumeAttachmentTemplate)
     def create(self, req, server_id, body):
         """Attach a volume to an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._attach_nbs_volume(req, server_id, body)
+
         context = req.environ['nova.context']
         authorize(context)
 
@@ -420,8 +526,46 @@ class VolumeAttachmentController(wsgi.Controller):
         """Update a volume attachment.  We don't currently support this."""
         raise exc.HTTPBadRequest()
 
+    def _detach_nbs_volume(self, req, server_id, id):
+        """Detach a nbs volume from an instance."""
+
+        context = req.environ['nova.context']
+        authorize(context)
+
+        volume_id = id
+        LOG.audit(_("Detach nbs volume %s from instance %s"),
+                    (volume_id, server_id), context=context)
+
+        try:
+            instance = self.compute_api.get(context, server_id)
+        except exception.NotFound:
+            raise exc.HTTPNotFound()
+
+        bdms = self.compute_api.get_instance_bdms(context, instance)
+
+        if not bdms:
+            LOG.debug(_("Instance %s is not attached."), server_id)
+            raise exc.HTTPNotFound()
+
+        found = False
+        for bdm in bdms:
+            if bdm['volume_id'] == volume_id:
+                self.compute_api.detach_nbs_volume(context,
+                    instance, volume_id)
+                found = True
+                break
+
+        if not found:
+            raise exc.HTTPNotFound()
+        else:
+            return webob.Response(status_int=202)
+
     def delete(self, req, server_id, id):
         """Detach a volume from an instance."""
+        # Go to our owned process if we are attaching a nbs volume
+        if FLAGS.ebs_backend == 'nbs':
+            return self._detach_nbs_volume(req, server_id, id)
+
         context = req.environ['nova.context']
         authorize(context)
 
diff --git a/nova/compute/api.py b/nova/compute/api.py
index bd417fe..b6a6d67 100644
--- a/nova/compute/api.py
+++ b/nova/compute/api.py
@@ -22,12 +22,15 @@
 networking and storage of VMs, and compute hosts on which they run)."""
 
 import base64
+import datetime
 import functools
 import re
 import string
 import time
 import urllib
 
+import memcache
+
 from nova import block_device
 from nova.compute import instance_types
 from nova.compute import power_state
@@ -63,6 +66,7 @@ flags.DECLARE('consoleauth_topic', 'nova.consoleauth')
 
 MAX_USERDATA_SIZE = 65535
 QUOTAS = quota.QUOTAS
+MEMCACHE_CLIENT_API = None
 
 
 def check_instance_state(vm_state=None, task_state=(None,)):
@@ -2104,6 +2108,87 @@ class API(base.Base):
         """Inject network info for the instance."""
         self.compute_rpcapi.inject_network_info(context, instance=instance)
 
+    def _get_memcache_client(self):
+        """Return memcache client"""
+        global MEMCACHE_CLIENT_API
+        if MEMCACHE_CLIENT_API is not None:
+            return MEMCACHE_CLIENT_API
+        else:
+            if FLAGS.memcached_servers:
+                MEMCACHE_CLIENT_API = memcache.Client(FLAGS.memcached_servers,
+                                                      debug=0)
+            return MEMCACHE_CLIENT_API
+
+    def instance_os_boot_ready(self, context, instance_uuid, heartbeat_period):
+        #NOTE(hzyangtk): determine VM is active or not by VM heartbeat
+        status = 'down'
+        interval = heartbeat_period * 1.5
+
+        cache_key = str(instance_uuid + '_heart')
+        memcache_client = self._get_memcache_client()
+        if memcache_client is not None:
+            cache_value = memcache_client.get(cache_key)
+            if cache_value:
+                last_heartbeat = datetime.datetime.strptime(
+                                        cache_value,
+                                        '%Y-%m-%d %H:%M:%S')
+                # Timestamps in DB are UTC.
+                elapsed = utils.total_seconds(
+                                timeutils.utcnow() - last_heartbeat)
+                if abs(elapsed) <= interval:
+                    status = 'up'
+        else:
+            msg = _('Store data can not catch')
+            raise exc.HTTPServerError(explanation=msg)
+
+        return {'instance_uuid': instance_uuid,
+                'status': status}
+
+    @wrap_check_policy#####
+    @check_instance_lock
+    def check_nbs_attached(self, context, volume_id):
+        """ """
+        # call nbs to get the volume info
+        try:
+            volume = self.nbs_client.get(context, volume_id)
+        except notfound:
+            raise notfound
+        if volume['status'] != "attachedVM":
+            return volume['instanceId']
+        else:
+            return None
+
+    @wrap_check_policy#####
+    @check_instance_lock
+    def extend_nbs_volume(self, context, instance, volume_id, size):
+        """ """
+        self.compute_rpcapi.extend_nbs_volume(context, instance=instance,
+                    volume_id=volume_id, size=size, device=None)
+
+    @wrap_check_policy#####
+    @check_instance_lock
+    def attach_nbs_volume(self, context, instance, volume_id):
+        """Attach an existing volume to an existing instance."""
+        # NOTE(vish): This is done on the compute host because we want
+        #             to avoid a race where two devices are requested at
+        #             the same time. When db access is removed from
+        #             compute, the bdm will be created here and we will
+        #             have to make sure that they are assigned atomically.
+        device = self.compute_rpcapi.get_device_for_nbs_volume(
+                        context, instance=instance)
+        if device is None:
+            # can't find a slot or target dev to attach
+            raise xxxxx
+        #try:
+            self.compute_rpcapi.attach_nbs_volume(context, instance=instance,
+                    volume_id=volume_id, device=device)
+        #except Exception:
+        #    with excutils.save_and_reraise_exception():
+        #        self.db.block_device_mapping_destroy_by_instance_and_device(
+        #                context, instance['uuid'], device)
+
+        return device['real_path']
+
     @wrap_check_policy
     @check_instance_lock
     def attach_volume(self, context, instance, volume_id, device=None):
@@ -2135,6 +2220,22 @@ class API(base.Base):
         return device
 
     @check_instance_lock
+    def detach_nbs_volume(self, context, instance, volume_id):
+        """Detach a volume from an instance."""
+        volume = self.nbs_client.get(context, volume_id)
+        # FIXME(wangpan): how to deal with 'attached'?
+        if volume['status'] != 'attachedVM':
+            msg = _("Volume must be attached in order to detach.")
+            raise exception.InvalidVolume(reason=msg)
+
+        if volume['instance_uuid'] != instance['uuid']:
+            raise exception.VolumeUnattached(volume_id=volume_id)
+
+        check_policy(context, 'detach_volume', instance)
+        self.compute_rpcapi.detach_nbs_volume(context, instance=instance,
+                volume_id=volume_id)
+
+    @check_instance_lock
     def _detach_volume(self, context, instance, volume_id):
         check_policy(context, 'detach_volume', instance)
 
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index e863e73..afc13f2 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -2878,6 +2878,23 @@ class ComputeManager(manager.SchedulerDependentManager):
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
+    def get_device_for_nbs_volume(self, context, instance, used_dev=None):
+
+        @utils.synchronized(instance['uuid'])
+        def do_reserve():
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': jsonutils.dumps(result)}
+            self.db.block_device_mapping_create(context, values)
+            return result
+        return do_reserve()
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
     def reserve_block_device_name(self, context, instance, device):
 
         @utils.synchronized(instance['uuid'])
@@ -2895,6 +2912,117 @@ class ComputeManager(manager.SchedulerDependentManager):
     @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
     @reverts_task_state
     @wrap_instance_fault
+    def extend_nbs_volume(self, context, instance, volume_id, size, device):
+        """Extend a nbs volume which has been attached on an instance."""
+        # Get attached device from nova db
+        if device is None:
+            bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                                volume_id)
+            device = jsonutils.loads(bdm['device_name'])
+        # TODO(wangpan): if this host can't attach nbs volume, raise exception
+        try:
+            return self.driver.extend_nbs_volume(instance, device, size)
+        except Exception:
+            CCXXXX
+            # notify nbs extend failed
+        else:
+            # notify nbs extend OK
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def attach_nbs_volume(self, context, volume_id, device, instance):
+        """Attach a nbs volume to an instance."""
+        # TODO(wangpan): if this host can't attach nbs volume, raise exception
+        try:
+            return self._attach_nbs_volume(context, volume_id,
+                                           device, instance)
+        except Exception:
+            with excutils.save_and_reraise_exception():
+                self.db.block_device_mapping_destroy_by_instance_and_device(
+                        context, instance.get('uuid'), jsonutils.dumps(device))
+
+    def _attach_nbs_volume(self, context, volume_id, device, instance):
+        context = context.elevated()
+        LOG.audit(_('Attaching nbs volume %(volume_id)s to %(device)s'),
+                  locals(), context=context, instance=instance)
+        try:
+            # call nbs to attach volume to host
+
+            # check volume status
+
+            # get host dev path and QoS params from nbs
+
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to connect to volume %(volume_id)s "
+                        "while attaching at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+
+        @utils.synchronized(instance['uuid'])
+        def get_new_device(used_dev):
+            result = self.driver.get_device_for_nbs_volume(instance['name'],
+                                                           used_dev)
+
+            # NOTE(vish): create bdm here to avoid race condition
+            values = {'instance_uuid': instance['uuid'],
+                      'device_name': result}
+            self.db.block_device_mapping_update_or_create(context, values)
+            return result
+
+        try:
+            used_dev = {'dev': [], 'slot': []}
+            retry = True
+            while retry:
+                retry, used = self.driver.attach_nbs_volume(instance['name'],
+                                    device, host_dev, qos_info, volume_id)
+                if retry:
+                    if used['dev']:
+                        used_dev['dev'].append(used['dev'])
+                    elif used['slot']:
+                        used_dev['slot'].append(used['slot'])
+                    device = get_new_device(used_dev)
+                    if device is None:
+                        # can't find a slot or target dev to attach
+                        raise xxxx
+                    LOG.info(_("Retry to attach nbs volume %s at %s"
+                            % (volume_id, device),
+                            context=context, instance=instance)
+#        except notfound:
+#            # instance disappear
+#            LOG.error(_("Instance disappeared during attaching nbs volume %s "
+#                        "at %s" % (volume_id, device),
+ #                       context=context, instance=instance)
+#            # notify nbs attach failed
+#
+ #           return
+        except Exception:  # pylint: disable=W0702
+            with excutils.save_and_reraise_exception():
+                msg = _("Failed to attach nbs volume %(volume_id)s "
+                        "at %(device)s")
+                LOG.exception(msg % locals(), context=context,
+                              instance=instance)
+                # notify nbs attach failed
+        else:
+            # notify nbs attach successfully
+
+        values = {
+            'instance_uuid': instance['uuid'],
+            'connection_info': None,
+            'device_name': jsonutils.dumps(device),
+            'delete_on_termination': False,
+            'virtual_name': None,
+            'snapshot_id': None,
+            'volume_id': volume_id,
+            'volume_size': None,
+            'no_device': None}
+        self.db.block_device_mapping_update_or_create(context, values)
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
     def attach_volume(self, context, volume_id, mountpoint, instance):
         """Attach a volume to an instance."""
         try:
@@ -2956,6 +3084,35 @@ class ComputeManager(manager.SchedulerDependentManager):
             'no_device': None}
         self.db.block_device_mapping_update_or_create(context, values)
 
+    def _detach_nbs_volume(self, context, instance, bdm):
+        """Do the actual driver detach using block device mapping."""
+        mp = jsonutils.loads(bdm['device_name'])['mountpoint']
+        volume_id = bdm['volume_id']
+
+        LOG.audit(_('Detach volume %(volume_id)s from mountpoint %(mp)s'),
+                  locals(), context=context, instance=instance)
+
+        try:
+            self.driver.detach_nbs_volume(instance['name'], mp)
+        except Exception:  # pylint: disable=W0702
+            msg = _("Faild to detach volume %(volume_id)s from %(mp)s")
+            LOG.exception(msg % locals(), context=context, instance=instance)
+
+    @exception.wrap_exception(notifier=notifier, publisher_id=publisher_id())
+    @reverts_task_state
+    @wrap_instance_fault
+    def detach_nbs_volume(self, context, volume_id, instance):
+        """Detach a nbs volume from an instance."""
+        bdm = self._get_instance_volume_bdm(context, instance['uuid'],
+                                            volume_id)
+        self._detach_nbs_volume(context, instance, bdm)
+        # call nbs to detach volume from host
+        self.nbs_client.detach(context, volume)
+        # check result and log it
+        if succ:
+            self.db.block_device_mapping_destroy_by_instance_and_volume(
+                    context, instance['uuid'], volume_id)
+
     def _detach_volume(self, context, instance, bdm):
         """Do the actual driver detach using block device mapping."""
         mp = bdm['device_name']
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index 0cd2668..327ca16 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -170,6 +170,19 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 instance=instance_p, network_id=network_id),
                 topic=_compute_topic(self.topic, ctxt, None, instance))
 
+    def extend_nbs_volume(self, ctxt, instance, device=None):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('extend_nbs_volume',
+                instance=instance_p, volume_id=volume_id,
+                size=size, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
+    def attach_nbs_volume(self, ctxt, instance, volume_id, device):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('attach_nbs_volume',
+                instance=instance_p, volume_id=volume_id, device=device),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def attach_volume(self, ctxt, instance, volume_id, mountpoint):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('attach_volume',
@@ -210,6 +223,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
                 reservations=reservations),
                 topic=_compute_topic(self.topic, ctxt, host, instance))
 
+    def detach_nbs_volume(self, ctxt, instance, volume_id):
+        instance_p = jsonutils.to_primitive(instance)
+        self.cast(ctxt, self.make_msg('detach_nbs_volume',
+                instance=instance_p, volume_id=volume_id),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def detach_volume(self, ctxt, instance, volume_id):
         instance_p = jsonutils.to_primitive(instance)
         self.cast(ctxt, self.make_msg('detach_volume',
@@ -458,6 +477,12 @@ class ComputeAPI(nova.openstack.common.rpc.proxy.RpcProxy):
         topic = _compute_topic(self.topic, ctxt, host, None)
         return self.call(ctxt, self.make_msg('get_host_uptime'), topic)
 
+    def get_device_for_nbs_volume(self, ctxt, instance):
+        instance_p = jsonutils.to_primitive(instance)
+        return self.call(ctxt, self.make_msg('get_device_for_nbs_volume',
+                instance=instance_p),
+                topic=_compute_topic(self.topic, ctxt, None, instance))
+
     def reserve_block_device_name(self, ctxt, instance, device):
         instance_p = jsonutils.to_primitive(instance)
         return self.call(ctxt, self.make_msg('reserve_block_device_name',
diff --git a/nova/compute/utils.py b/nova/compute/utils.py
index a8a0a65..f331510 100644
--- a/nova/compute/utils.py
+++ b/nova/compute/utils.py
@@ -55,7 +55,6 @@ def add_instance_fault_from_exc(context, instance_uuid, fault, exc_info=None):
     }
     db.instance_fault_create(context, values)
 
-
 def get_device_name_for_instance(context, instance, device):
     """Validates (or generates) a device name for instance.
 
diff --git a/nova/flags.py b/nova/flags.py
index c0112ce..934f1b5 100644
--- a/nova/flags.py
+++ b/nova/flags.py
@@ -230,6 +230,31 @@ resize_opts = [
 
 FLAGS.register_opts(resize_opts)
 
+nbs_opts = [
+    cfg.StrOpt('ebs_backend',
+                default='nbs',
+                help='the backend type of ebs service, may be nbs or cinder'),
+    cfg.StrOpt('nbs_api_server',
+                default=None,
+                help='the host and port of nbs server'),
+    cfg.StrOpt('nbs_prefix_url',
+                default='EBS',
+                help='the url prefix of nbs server, the final url should like '
+                 'this: nbs_api_server/nbs_prefix_url/?Action=XX&YY=ZZ...'),
+    cfg.StrOpt('host_ip_ifname',
+               default='br100',
+               help='network device to get intranet ip address for nbs'),
+    cfg.IntOpt('nbs_wait_timeout',
+               default='br100',
+               help='network device to get intranet ip address for nbs'),
+    cfg.StrOpt('nbs_mountpoint_prefix',
+               default='nbs',
+               help='mount point prefix of nbs disk displays in vm, may like '
+                    '/dev/nbs/xd'),
+]
+
+FLAGS.register_opts(nbs_opts)
+
 global_opts = [
     cfg.StrOpt('my_ip',
                default=_get_my_ip(),
diff --git a/nova/virt/driver.py b/nova/virt/driver.py
index 8be6ff7..293c150 100644
--- a/nova/virt/driver.py
+++ b/nova/virt/driver.py
@@ -233,10 +233,23 @@ class ComputeDriver(object):
         # TODO(Vek): Need to pass context in for access to auth_token
         raise NotImplementedError()
 
+    def extend_nbs_volume(self, instance_name, device, size):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """Attach the disk to the instance at mountpoint using info"""
+        raise NotImplementedError()
+
     def attach_volume(self, connection_info, instance_name, mountpoint):
         """Attach the disk to the instance at mountpoint using info"""
         raise NotImplementedError()
 
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        """Detach the disk attached to the instance"""
+        raise NotImplementedError()
+
     def detach_volume(self, connection_info, instance_name, mountpoint):
         """Detach the disk attached to the instance"""
         raise NotImplementedError()
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 5bd7529..89164e0 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -734,6 +734,137 @@ class LibvirtDriver(driver.ComputeDriver):
         method = getattr(driver, method_name)
         return method(connection_info, *args, **kwargs)
 
+    def _slot_to_disk(self, slot):
+        ascii_table = "abcdefghijklmnopqrstuvwxyz"
+        nums = slot
+        nbs_disk = ''
+        while nums > 0:
+            value = nums % 26
+            ch = ascii_table[value:value + 1]
+            nbs_disk = ch + nbs_disk
+            nums = nums / 26
+        return nbs_disk
+
+    # FIXME(wangpan): we find that only slot id is non-zero in pci address,
+    #                 such as: <address type='pci'
+    #                           domain='0x0000'
+    #                           bus='0x00'
+    #                           slot='0x03'
+    #                           function='0x0'/>
+    #                 so we only find a free slot of domain.
+    def _find_free_dev_and_pcislot(self, instance_name, used_dev):
+        virt_dom = self._lookup_by_name(instance_name)
+        dom_xml = virt_dom.XMLDesc(0)
+        doc = etree.fromstring(dom_xml)
+        devices = doc.findall('./devices/')
+        used_pci_slot_ids = [int(device.find('address').attrib['slot'], 16)
+                                for device in devices
+                                    if device.find('address') is not None]
+        disks = doc.findall('./devices/disk')
+        used_disk_devs = [disk.find('target').attrib['dev'] for disk in disks]
+
+        free_slot = None
+        free_dev = None
+
+        # slot id range is 0-31
+        for id in range(31, 0, -1):
+            if id not in used_pci_slot_ids and id not in used_dev['slot']:
+                free_slot = id
+
+        # vd[c-z]
+        for i in range(99, 123):
+            dev = 'vd' + chr(i)
+            if dev not in used_disk_devs and dev not in used_dev['dev']:
+                free_dev = dev
+
+        return (free_slot, free_dev)
+
+
+    def get_device_for_nbs_volume(instance_name, used_dev):
+        """Generates a device name for attaching nbs volume"""
+        free_slot, free_dev = self._find_free_dev_and_pcislot(instance_name,
+                                                                used_dev)
+        if free_slot is None or free_dev is None:
+            return None
+
+        real_path = FLAGS.nbs_mountpoint_prefix + self._slot_to_disk(free_slot)
+        target_dev = '/dev/' + free_dev
+
+        return {'mountpoint': target_dev,
+                'real_path': real_path,
+                'slot': free_slot}
+
+    @exception.wrap_exception()
+    def extend_nbs_volume(self, instance_name, device, size):
+        """ """
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise notsupported
+
+        mount_device = device['mountpoint'].rpartition("/")[2]
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                size_KB = size * 1024 * 1024
+                virt_dom.blockResize(mount_device, size_KB, 0)
+        except CCCXXX:
+            raise XXX
+
+    @exception.wrap_exception()
+    def attach_nbs_volume(self, instance_name, device, host_dev,
+                          qos_info, volume_id):
+        """
+            return retry or not, if need retry, used device is returned, too.
+        """
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise notsupported
+
+        mount_device = device['mountpoint'].rpartition("/")[2]
+        slot = device['slot']
+        iotune_total_bytes = qos_info['iotune_total_bytes']
+        iotune_total_iops = qos_info['iotune_total_iops']
+        disk_xml = '''<disk type='block'>
+                        <driver name='qemu' type='raw' cache='none'/>
+                        <source dev='%s'/>
+                        <target dev='%s' bus='virtio'/>
+                        <address type='pci' domain='0x0000' \
+                         bus='0x00' slot='0x%0.2x' function='0x0'/>
+                        <iotune>
+                          <total_bytes_sec>%d</total_bytes_sec>
+                          <total_iops_sec>%d</total_iops_sec>
+                        </iotune>
+                        <serial>%s</serial>
+                      </disk>''' % (host_dev, mount_device, slot,
+                                    iotune_total_bytes, iotune_total_iops,
+                                    volume_id)
+
+        virt_dom = self._lookup_by_name(instance_name)
+        try:
+            # NOTE(vish): We can always affect config because our
+            #             domains are persistent, but we should only
+            #             affect live if the domain is running.
+            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+            if state == power_state.RUNNING:
+                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+            virt_dom.attachDeviceFlags(disk_xml, flags)
+            return (False, None)
+        except Exception as ex:
+            if isinstance(ex, libvirt.libvirtError):
+                errcode = ex.get_error_code()
+                if errcode == libvirt.VIR_ERR_OPERATION_FAILED:
+                    raise exception.DeviceIsBusy(device=disk_dev)
+                elif errcode == libvirt.VIR_ERR_INVALID_ARG:
+                    # target dev already exists
+                    return (True, {'dev': mount_device})
+                elif errcode == libvirt.VIR_ERR_XML_ERROR:
+                    # pci slot already used
+                    return (True, {'slot': slot})
+            else:
+                raise ex
+
     @exception.wrap_exception()
     def attach_volume(self, connection_info, instance_name, mountpoint):
         virt_dom = self._lookup_by_name(instance_name)
@@ -792,6 +923,38 @@ class LibvirtDriver(driver.ComputeDriver):
         return xml
 
     @exception.wrap_exception()
+    def detach_nbs_volume(self, instance_name, mountpoint):
+        if FLAGS.libvirt_type == 'lxc':
+            LOG.error(_("LXC hypervisor is not supported to attach volume."))
+            raise notsupported
+
+        mount_device = mountpoint.rpartition("/")[2]
+        try:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still logout even if
+            #             the instance doesn't exist here anymore.
+            virt_dom = self._lookup_by_name(instance_name)
+            xml = self._get_disk_xml(virt_dom.XMLDesc(0), mount_device)
+            if not xml:
+                raise exception.DiskNotFound(location=mount_device)
+            else:
+                flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
+                state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
+                if state == power_state.RUNNING:
+                    flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
+                virt_dom.detachDeviceFlags(xml, flags)
+        except libvirt.libvirtError as ex:
+            # NOTE(vish): This is called to cleanup volumes after live
+            #             migration, so we should still disconnect even if
+            #             the instance doesn't exist here anymore.
+            error_code = ex.get_error_code()
+            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
+                # NOTE(vish):
+                LOG.warn(_("During detach_volume, instance disappeared."))
+            else:
+                raise
+
+    @exception.wrap_exception()
     def detach_volume(self, connection_info, instance_name, mountpoint):
         mount_device = mountpoint.rpartition("/")[2]
         try:
